{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DQN\n",
    "import keras\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import gymnasium as gym\n",
    "import os\n",
    "import tensorflow as tf\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "use the dense to generate the action\n",
    "\"\"\"\n",
    "\n",
    "class Network(keras.Model):\n",
    "    def __init__(self, actionNum, stateShape, learningRate):\n",
    "        super(Network, self).__init__()\n",
    "\n",
    "        self.actionNum = actionNum\n",
    "        self.stateShape = stateShape\n",
    "\n",
    "        self.model = keras.Sequential(\n",
    "            [\n",
    "                keras.layers.Dense(32, input_shape=stateShape),\n",
    "                keras.layers.LeakyReLU(),\n",
    "                keras.layers.Dense(64),\n",
    "                keras.layers.LeakyReLU(),\n",
    "                keras.layers.Dense(actionNum),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # self.model.compile(\n",
    "        #     loss = 'mse',\n",
    "        #     optimizer = keras.optimizers.Adam(learningRate)\n",
    "        # )\n",
    "\n",
    "        self.lossFunc = keras.losses.mean_squared_error\n",
    "        self.optimizer = keras.optimizers.Adam(learningRate)\n",
    "\n",
    "    def call(self, state):\n",
    "        return self.model(state, training=False)\n",
    "\n",
    "    # @tf.function\n",
    "    # def Update(self, lastState, nextState, targetNetwork, reward, gamma):\n",
    "    #     with tf.GradientTape() as tape:\n",
    "    #         evaluateValue = tf.reduce_max(self.model(lastState, training=True).numpy(), axis = 1)\n",
    "    #         targetValue = targetNetwork(nextState, training=False)\n",
    "\n",
    "    #         targetValue = reward + gamma * np.max(targetValue.numpy(), axis=1)\n",
    "    #         print(evaluateValue.shape, targetValue.shape)\n",
    "    #         # //TODO 修改梯度下降法\n",
    "    #         loss = self.lossFunc(evaluateValue, targetValue)\n",
    "    #     gradients = tape.gradient(loss, self.model.trainable_variables)\n",
    "    #     self.optimizer.apply_gradients(zip(gradients, self.model.trainable_variables))\n",
    "\n",
    "    #     return sum(loss)\n",
    "\n",
    "    def Update(self, lastState, action, targetValue):\n",
    "        loss = 0\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            evaluateValue = tf.math.reduce_sum(\n",
    "                self.model(lastState) * tf.one_hot(action, self.actionNum),\n",
    "                axis = 1\n",
    "            )\n",
    "            loss = tf.math.reduce_mean(tf.square(evaluateValue - targetValue))\n",
    "        \n",
    "        gradients = tape.gradient(loss, self.model.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(gradients, self.model.trainable_variables))  \n",
    "\n",
    "        return loss\n",
    "\n",
    "    def CopyVariable(self, network):\n",
    "        \"\"\"\n",
    "        param:\n",
    "            network:    the other network model\n",
    "                type:   Sequencial\n",
    "        func:\n",
    "            Copy the variable of other network model to self\n",
    "        \"\"\"\n",
    "        for selfLayer, otherLayer in zip(self.model.layers, network.layers):\n",
    "            selfLayer.set_weights(otherLayer.get_weights())\n",
    "\n",
    "\n",
    "class DQN:\n",
    "    def __init__(\n",
    "        self,\n",
    "        actionNum,\n",
    "        stateShape,\n",
    "        epsilon,\n",
    "        gamma,\n",
    "        batchSize,\n",
    "        poolSize,\n",
    "        learningRate,\n",
    "        environment,\n",
    "        startCount,\n",
    "        learningCut,\n",
    "        updateTargetCut,\n",
    "        savePath,\n",
    "        saveName,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        param:\n",
    "            actionNum:          the number of action\n",
    "            stateShape:         the shape of state\n",
    "            epsilon:            the parameter which is used in e-Greedy\n",
    "            gamma:              used in the loss of Q Max\n",
    "            batchSize:          the batch size\n",
    "            poolSize:           the size of memory pool\n",
    "            learningRate:       for optimize the network\n",
    "            environment:        the game which network will learn to play\n",
    "            startCount:         the step when network start to update\n",
    "            learningCut:        after one episode's step reach to the startCount, every learningCut we will update the parameter of network\n",
    "            updateTargetCut:  when reach this number, copy the weight of ChooseActionNetwork to the TargetNetwork\n",
    "\n",
    "            savePath:           the dir model will be saved\n",
    "            saveName:           the name model will be saved\n",
    "        \"\"\"\n",
    "\n",
    "        self.environment = environment\n",
    "\n",
    "        self.actionNum = actionNum\n",
    "        self.stateShape = stateShape\n",
    "        self.epsilon = epsilon\n",
    "        self.gamma = gamma\n",
    "        self.batchSize = batchSize\n",
    "        self.poolSize = poolSize\n",
    "\n",
    "        self.chooseActionNet = Network(actionNum, stateShape, learningRate)\n",
    "        self.targetNetwork = Network(actionNum, stateShape, learningRate)\n",
    "\n",
    "        self.memoryPool = {\n",
    "            \"state\": [],\n",
    "            \"action\": [],\n",
    "            \"reward\": [],\n",
    "            \"nextState\": [],\n",
    "        }\n",
    "\n",
    "        # self.memoryPool = []\n",
    "        self.memoryLength = 0\n",
    "\n",
    "        self.startCount = startCount\n",
    "        self.learningCut = learningCut\n",
    "        self.updateTargetCut = updateTargetCut\n",
    "\n",
    "        self.savePath = savePath\n",
    "        self.saveName = saveName\n",
    "\n",
    "        # to store the rewards\n",
    "        self.rewardsStore = []\n",
    "\n",
    "        # if not os.path.exists(savePath):\n",
    "        #     os.mkdir(savePath)\n",
    "\n",
    "    def ChooseAction(self, state):\n",
    "        if np.random.uniform() > self.epsilon:\n",
    "            return np.random.randint(self.actionNum)\n",
    "\n",
    "        predictValue = self.chooseActionNet.call(state)\n",
    "        return np.argmax(predictValue)\n",
    "\n",
    "    def StoreMemory(self, lastState, action, reward, nextState):\n",
    "        self.memoryPool[\"state\"].append(np.array(lastState))\n",
    "        self.memoryPool[\"action\"].append(np.array(action))\n",
    "        self.memoryPool[\"reward\"].append(np.array(reward))\n",
    "        self.memoryPool[\"nextState\"].append(np.array(nextState))\n",
    "\n",
    "        if self.memoryLength >= self.poolSize:\n",
    "            self.memoryPool[\"state\"].pop(0)\n",
    "            self.memoryPool[\"action\"].pop(0)\n",
    "            self.memoryPool[\"reward\"].pop(0)\n",
    "            self.memoryPool[\"nextState\"].pop(0)\n",
    "        else:\n",
    "            self.memoryLength += 1\n",
    "        # self.memoryPool.append(\n",
    "        #     (lastState, action, reward, nextState)\n",
    "        # )\n",
    "\n",
    "    def ChooseMemory(self):\n",
    "        \"\"\"\n",
    "        func:\n",
    "            Select a batchSize of observations from the memoryPool\n",
    "        return:\n",
    "            laststate\n",
    "            action\n",
    "            reward\n",
    "            nextState\n",
    "        \"\"\"\n",
    "\n",
    "        # 生成需要的 index\n",
    "        index = np.random.choice(self.memoryLength - 1, size = self.batchSize)\n",
    "        laststate = []\n",
    "        action = []\n",
    "        reward = []\n",
    "        nextState = []\n",
    "\n",
    "        for idx in index:\n",
    "            laststate.append(self.memoryPool[\"state\"][idx])\n",
    "            action.append(self.memoryPool[\"action\"][idx])\n",
    "            reward.append(self.memoryPool[\"reward\"][idx])\n",
    "            nextState.append(self.memoryPool[\"nextState\"][idx])\n",
    "\n",
    "        return (\n",
    "            np.array(laststate),\n",
    "            np.array(action),\n",
    "            np.array(reward),\n",
    "            np.array(nextState),\n",
    "        )\n",
    "\n",
    "    def UpdateNetwork(self):\n",
    "        \"\"\"\n",
    "        param:\n",
    "\n",
    "        func:\n",
    "            use the memory to make the batch to feed the network\n",
    "        \"\"\"\n",
    "\n",
    "        lastState, action, reward, nextState = self.ChooseMemory()\n",
    "\n",
    "        # history = self.chooseActionNet.model.fit(evaluation, target)\n",
    "        # loss = self.chooseActionNet.Update(\n",
    "        #     lastState, nextState, self.targetNetwork.model, reward, self.gamma\n",
    "        # )\n",
    "\n",
    "        # targetValue = self.chooseActionNet.model.predict(lastState, verbose = 0)\n",
    "\n",
    "        # target = self.targetNetwork.model.predict(nextState, verbose = 0)\n",
    "        # targetValue[range(self.batchSize), action] = reward + self.gamma * np.max(target, axis = 1)\n",
    "        \n",
    "        # history = self.chooseActionNet.model.fit(lastState, targetValue, verbose = 0, epochs = 5)\n",
    "\n",
    "        # return sum(history.history['loss']) / float(self.batchSize)\n",
    "\n",
    "\n",
    "        targetValue = reward + self.gamma * np.max(self.targetNetwork.model.predict(nextState, verbose = 0), axis = 1)\n",
    "\n",
    "        return self.chooseActionNet.Update(lastState, action, targetValue)\n",
    "\n",
    "    def CopyNetwork(self):\n",
    "        \"\"\"\n",
    "        while copies the variable, this function will store the weight to the disk\n",
    "        \"\"\"\n",
    "\n",
    "        self.targetNetwork.CopyVariable(self.chooseActionNet.model)\n",
    "\n",
    "        # Save Model\n",
    "        self.targetNetwork.model.save_weights(self.savePath + self.saveName)\n",
    "\n",
    "    def Train(self):\n",
    "        isDone = False\n",
    "        lastState, _ = self.environment.reset()\n",
    "        learningCount = 0\n",
    "        afterCount = 0\n",
    "\n",
    "        rewardSum = 0\n",
    "        loss = 0\n",
    "        while not isDone:\n",
    "            action = self.ChooseAction(np.array([lastState]))\n",
    "\n",
    "            nextState, reward, isDone, _, _ = self.environment.step(action)\n",
    "\n",
    "            self.StoreMemory(lastState, action, reward, nextState)\n",
    "\n",
    "            if learningCount > self.startCount:\n",
    "                if afterCount % self.learningCut == 0:\n",
    "                    loss += self.UpdateNetwork()\n",
    "\n",
    "                if afterCount % self.updateTargetCut == 0:\n",
    "                    self.CopyNetwork()\n",
    "\n",
    "                afterCount += 1\n",
    "\n",
    "            lastState = nextState\n",
    "            learningCount += 1\n",
    "            rewardSum += reward\n",
    "\n",
    "        self.rewardsStore.append(rewardSum)\n",
    "        return rewardSum, learningCount, loss / learningCount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the super parameter\n",
    "env = gym.make(\"LunarLander-v2\")\n",
    "ACTIONNUM = env.action_space.n\n",
    "STATESHAPE = env.observation_space.shape\n",
    "EPSILON = 0.9\n",
    "GAMMA = 0.9\n",
    "STARTCOUNT = 10\n",
    "LEARNINGCUT = 3\n",
    "UPDATETARGETCUT = 10\n",
    "BATCHSIZE = 16\n",
    "POOLSIZE = 200\n",
    "LEARNINGRATE = 0.3\n",
    "SAVEPATH = \"../SavedModel/RL/DQN/\"\n",
    "SAVENAME = \"DQN1weight.h5\"\n",
    "EPOCHS = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32md:\\AI\\ReinforcementLearning\\DQN1.ipynb Cell 4\u001b[0m line \u001b[0;36m1\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/AI/ReinforcementLearning/DQN1.ipynb#W3sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m dqnModel \u001b[39m=\u001b[39m DQN(\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/AI/ReinforcementLearning/DQN1.ipynb#W3sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     ACTIONNUM,\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/AI/ReinforcementLearning/DQN1.ipynb#W3sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     STATESHAPE,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/AI/ReinforcementLearning/DQN1.ipynb#W3sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m     SAVENAME,\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/AI/ReinforcementLearning/DQN1.ipynb#W3sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m )\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/AI/ReinforcementLearning/DQN1.ipynb#W3sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1000\u001b[39m):\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/AI/ReinforcementLearning/DQN1.ipynb#W3sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m     reward, step, loss \u001b[39m=\u001b[39m dqnModel\u001b[39m.\u001b[39;49mTrain()\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/AI/ReinforcementLearning/DQN1.ipynb#W3sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mround: \u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m, reward: \u001b[39m\u001b[39m%.2f\u001b[39;00m\u001b[39m step: \u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m loss: \u001b[39m\u001b[39m%.2f\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m (i \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m, reward, step, loss))\n",
      "\u001b[1;32md:\\AI\\ReinforcementLearning\\DQN1.ipynb Cell 4\u001b[0m line \u001b[0;36m2\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/AI/ReinforcementLearning/DQN1.ipynb#W3sZmlsZQ%3D%3D?line=253'>254</a>\u001b[0m \u001b[39mif\u001b[39;00m learningCount \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstartCount:\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/AI/ReinforcementLearning/DQN1.ipynb#W3sZmlsZQ%3D%3D?line=254'>255</a>\u001b[0m     \u001b[39mif\u001b[39;00m afterCount \u001b[39m%\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlearningCut \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m--> <a href='vscode-notebook-cell:/d%3A/AI/ReinforcementLearning/DQN1.ipynb#W3sZmlsZQ%3D%3D?line=255'>256</a>\u001b[0m         loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mUpdateNetwork()\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/AI/ReinforcementLearning/DQN1.ipynb#W3sZmlsZQ%3D%3D?line=257'>258</a>\u001b[0m     \u001b[39mif\u001b[39;00m afterCount \u001b[39m%\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mupdateTargetCut \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/AI/ReinforcementLearning/DQN1.ipynb#W3sZmlsZQ%3D%3D?line=258'>259</a>\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mCopyNetwork()\n",
      "\u001b[1;32md:\\AI\\ReinforcementLearning\\DQN1.ipynb Cell 4\u001b[0m line \u001b[0;36m2\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/AI/ReinforcementLearning/DQN1.ipynb#W3sZmlsZQ%3D%3D?line=209'>210</a>\u001b[0m \u001b[39m# history = self.chooseActionNet.model.fit(evaluation, target)\u001b[39;00m\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/AI/ReinforcementLearning/DQN1.ipynb#W3sZmlsZQ%3D%3D?line=210'>211</a>\u001b[0m \u001b[39m# loss = self.chooseActionNet.Update(\u001b[39;00m\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/AI/ReinforcementLearning/DQN1.ipynb#W3sZmlsZQ%3D%3D?line=211'>212</a>\u001b[0m \u001b[39m#     lastState, nextState, self.targetNetwork.model, reward, self.gamma\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/AI/ReinforcementLearning/DQN1.ipynb#W3sZmlsZQ%3D%3D?line=220'>221</a>\u001b[0m \n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/AI/ReinforcementLearning/DQN1.ipynb#W3sZmlsZQ%3D%3D?line=221'>222</a>\u001b[0m \u001b[39m# return sum(history.history['loss']) / float(self.batchSize)\u001b[39;00m\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/AI/ReinforcementLearning/DQN1.ipynb#W3sZmlsZQ%3D%3D?line=224'>225</a>\u001b[0m targetValue \u001b[39m=\u001b[39m reward \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgamma \u001b[39m*\u001b[39m np\u001b[39m.\u001b[39mmax(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtargetNetwork\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mpredict(nextState, verbose \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m), axis \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m)\n\u001b[1;32m--> <a href='vscode-notebook-cell:/d%3A/AI/ReinforcementLearning/DQN1.ipynb#W3sZmlsZQ%3D%3D?line=226'>227</a>\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mchooseActionNet\u001b[39m.\u001b[39;49mUpdate(lastState, action, targetValue)\n",
      "\u001b[1;32md:\\AI\\ReinforcementLearning\\DQN1.ipynb Cell 4\u001b[0m line \u001b[0;36m5\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/AI/ReinforcementLearning/DQN1.ipynb#W3sZmlsZQ%3D%3D?line=55'>56</a>\u001b[0m     loss \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mmath\u001b[39m.\u001b[39mreduce_mean(tf\u001b[39m.\u001b[39msquare(evaluateValue \u001b[39m-\u001b[39m targetValue))\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/AI/ReinforcementLearning/DQN1.ipynb#W3sZmlsZQ%3D%3D?line=57'>58</a>\u001b[0m gradients \u001b[39m=\u001b[39m tape\u001b[39m.\u001b[39mgradient(loss, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mtrainable_variables)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/AI/ReinforcementLearning/DQN1.ipynb#W3sZmlsZQ%3D%3D?line=58'>59</a>\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptimizer\u001b[39m.\u001b[39;49mapply_gradients(\u001b[39mzip\u001b[39;49m(gradients, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49mtrainable_variables))  \n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/AI/ReinforcementLearning/DQN1.ipynb#W3sZmlsZQ%3D%3D?line=60'>61</a>\u001b[0m \u001b[39mreturn\u001b[39;00m loss\n",
      "File \u001b[1;32me:\\.scoop\\apps\\python310\\current\\lib\\site-packages\\keras\\src\\optimizers\\optimizer.py:1223\u001b[0m, in \u001b[0;36mOptimizer.apply_gradients\u001b[1;34m(self, grads_and_vars, name, skip_gradients_aggregation, **kwargs)\u001b[0m\n\u001b[0;32m   1221\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m skip_gradients_aggregation \u001b[39mand\u001b[39;00m experimental_aggregate_gradients:\n\u001b[0;32m   1222\u001b[0m     grads_and_vars \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maggregate_gradients(grads_and_vars)\n\u001b[1;32m-> 1223\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mapply_gradients(grads_and_vars, name\u001b[39m=\u001b[39;49mname)\n",
      "File \u001b[1;32me:\\.scoop\\apps\\python310\\current\\lib\\site-packages\\keras\\src\\optimizers\\optimizer.py:652\u001b[0m, in \u001b[0;36m_BaseOptimizer.apply_gradients\u001b[1;34m(self, grads_and_vars, name)\u001b[0m\n\u001b[0;32m    650\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_apply_weight_decay(trainable_variables)\n\u001b[0;32m    651\u001b[0m grads_and_vars \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mzip\u001b[39m(grads, trainable_variables))\n\u001b[1;32m--> 652\u001b[0m iteration \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_internal_apply_gradients(grads_and_vars)\n\u001b[0;32m    654\u001b[0m \u001b[39m# Apply variable constraints after applying gradients.\u001b[39;00m\n\u001b[0;32m    655\u001b[0m \u001b[39mfor\u001b[39;00m variable \u001b[39min\u001b[39;00m trainable_variables:\n",
      "File \u001b[1;32me:\\.scoop\\apps\\python310\\current\\lib\\site-packages\\keras\\src\\optimizers\\optimizer.py:1253\u001b[0m, in \u001b[0;36mOptimizer._internal_apply_gradients\u001b[1;34m(self, grads_and_vars)\u001b[0m\n\u001b[0;32m   1249\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_mesh \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_run_with_dtensor:\n\u001b[0;32m   1250\u001b[0m     \u001b[39m# Skip any usage of strategy logic for DTensor\u001b[39;00m\n\u001b[0;32m   1251\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m_internal_apply_gradients(grads_and_vars)\n\u001b[1;32m-> 1253\u001b[0m \u001b[39mreturn\u001b[39;00m tf\u001b[39m.\u001b[39;49m__internal__\u001b[39m.\u001b[39;49mdistribute\u001b[39m.\u001b[39;49minterim\u001b[39m.\u001b[39;49mmaybe_merge_call(\n\u001b[0;32m   1254\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_distributed_apply_gradients_fn,\n\u001b[0;32m   1255\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_distribution_strategy,\n\u001b[0;32m   1256\u001b[0m     grads_and_vars,\n\u001b[0;32m   1257\u001b[0m )\n",
      "File \u001b[1;32me:\\.scoop\\apps\\python310\\current\\lib\\site-packages\\tensorflow\\python\\distribute\\merge_call_interim.py:51\u001b[0m, in \u001b[0;36mmaybe_merge_call\u001b[1;34m(fn, strategy, *args, **kwargs)\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Maybe invoke `fn` via `merge_call` which may or may not be fulfilled.\u001b[39;00m\n\u001b[0;32m     32\u001b[0m \n\u001b[0;32m     33\u001b[0m \u001b[39mThe caller of this utility function requests to invoke `fn` via `merge_call`\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[39m  The return value of the `fn` call.\u001b[39;00m\n\u001b[0;32m     49\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     50\u001b[0m \u001b[39mif\u001b[39;00m strategy_supports_no_merge_call():\n\u001b[1;32m---> 51\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(strategy, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m   \u001b[39mreturn\u001b[39;00m distribute_lib\u001b[39m.\u001b[39mget_replica_context()\u001b[39m.\u001b[39mmerge_call(\n\u001b[0;32m     54\u001b[0m       fn, args\u001b[39m=\u001b[39margs, kwargs\u001b[39m=\u001b[39mkwargs)\n",
      "File \u001b[1;32me:\\.scoop\\apps\\python310\\current\\lib\\site-packages\\keras\\src\\optimizers\\optimizer.py:1345\u001b[0m, in \u001b[0;36mOptimizer._distributed_apply_gradients_fn\u001b[1;34m(self, distribution, grads_and_vars, **kwargs)\u001b[0m\n\u001b[0;32m   1342\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_update_step(grad, var)\n\u001b[0;32m   1344\u001b[0m \u001b[39mfor\u001b[39;00m grad, var \u001b[39min\u001b[39;00m grads_and_vars:\n\u001b[1;32m-> 1345\u001b[0m     distribution\u001b[39m.\u001b[39;49mextended\u001b[39m.\u001b[39;49mupdate(\n\u001b[0;32m   1346\u001b[0m         var, apply_grad_to_update_var, args\u001b[39m=\u001b[39;49m(grad,), group\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m\n\u001b[0;32m   1347\u001b[0m     )\n\u001b[0;32m   1349\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39muse_ema:\n\u001b[0;32m   1350\u001b[0m     _, var_list \u001b[39m=\u001b[39m \u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39mgrads_and_vars)\n",
      "File \u001b[1;32me:\\.scoop\\apps\\python310\\current\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:3011\u001b[0m, in \u001b[0;36mStrategyExtendedV2.update\u001b[1;34m(self, var, fn, args, kwargs, group)\u001b[0m\n\u001b[0;32m   3008\u001b[0m   fn \u001b[39m=\u001b[39m autograph\u001b[39m.\u001b[39mtf_convert(\n\u001b[0;32m   3009\u001b[0m       fn, autograph_ctx\u001b[39m.\u001b[39mcontrol_status_ctx(), convert_by_default\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m   3010\u001b[0m   \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_container_strategy()\u001b[39m.\u001b[39mscope():\n\u001b[1;32m-> 3011\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_update(var, fn, args, kwargs, group)\n\u001b[0;32m   3012\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   3013\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_replica_ctx_update(\n\u001b[0;32m   3014\u001b[0m       var, fn, args\u001b[39m=\u001b[39margs, kwargs\u001b[39m=\u001b[39mkwargs, group\u001b[39m=\u001b[39mgroup)\n",
      "File \u001b[1;32me:\\.scoop\\apps\\python310\\current\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:4081\u001b[0m, in \u001b[0;36m_DefaultDistributionExtended._update\u001b[1;34m(self, var, fn, args, kwargs, group)\u001b[0m\n\u001b[0;32m   4078\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_update\u001b[39m(\u001b[39mself\u001b[39m, var, fn, args, kwargs, group):\n\u001b[0;32m   4079\u001b[0m   \u001b[39m# The implementations of _update() and _update_non_slot() are identical\u001b[39;00m\n\u001b[0;32m   4080\u001b[0m   \u001b[39m# except _update() passes `var` as the first argument to `fn()`.\u001b[39;00m\n\u001b[1;32m-> 4081\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_update_non_slot(var, fn, (var,) \u001b[39m+\u001b[39;49m \u001b[39mtuple\u001b[39;49m(args), kwargs, group)\n",
      "File \u001b[1;32me:\\.scoop\\apps\\python310\\current\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:4087\u001b[0m, in \u001b[0;36m_DefaultDistributionExtended._update_non_slot\u001b[1;34m(self, colocate_with, fn, args, kwargs, should_group)\u001b[0m\n\u001b[0;32m   4083\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_update_non_slot\u001b[39m(\u001b[39mself\u001b[39m, colocate_with, fn, args, kwargs, should_group):\n\u001b[0;32m   4084\u001b[0m   \u001b[39m# TODO(josh11b): Figure out what we should be passing to UpdateContext()\u001b[39;00m\n\u001b[0;32m   4085\u001b[0m   \u001b[39m# once that value is used for something.\u001b[39;00m\n\u001b[0;32m   4086\u001b[0m   \u001b[39mwith\u001b[39;00m UpdateContext(colocate_with):\n\u001b[1;32m-> 4087\u001b[0m     result \u001b[39m=\u001b[39m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   4088\u001b[0m     \u001b[39mif\u001b[39;00m should_group:\n\u001b[0;32m   4089\u001b[0m       \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[1;32me:\\.scoop\\apps\\python310\\current\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py:596\u001b[0m, in \u001b[0;36mcall_with_unspecified_conversion_status.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    594\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapper\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m    595\u001b[0m   \u001b[39mwith\u001b[39;00m ag_ctx\u001b[39m.\u001b[39mControlStatusCtx(status\u001b[39m=\u001b[39mag_ctx\u001b[39m.\u001b[39mStatus\u001b[39m.\u001b[39mUNSPECIFIED):\n\u001b[1;32m--> 596\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32me:\\.scoop\\apps\\python310\\current\\lib\\site-packages\\keras\\src\\optimizers\\optimizer.py:1342\u001b[0m, in \u001b[0;36mOptimizer._distributed_apply_gradients_fn.<locals>.apply_grad_to_update_var\u001b[1;34m(var, grad)\u001b[0m\n\u001b[0;32m   1340\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_update_step_xla(grad, var, \u001b[39mid\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_var_key(var)))\n\u001b[0;32m   1341\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1342\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_update_step(grad, var)\n",
      "File \u001b[1;32me:\\.scoop\\apps\\python310\\current\\lib\\site-packages\\keras\\src\\optimizers\\optimizer.py:241\u001b[0m, in \u001b[0;36m_BaseOptimizer._update_step\u001b[1;34m(self, gradient, variable)\u001b[0m\n\u001b[0;32m    232\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_var_key(variable) \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_index_dict:\n\u001b[0;32m    233\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(\n\u001b[0;32m    234\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mThe optimizer cannot recognize variable \u001b[39m\u001b[39m{\u001b[39;00mvariable\u001b[39m.\u001b[39mname\u001b[39m}\u001b[39;00m\u001b[39m. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    235\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mThis usually means you are trying to call the optimizer to \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    239\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m`tf.keras.optimizers.legacy.\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    240\u001b[0m     )\n\u001b[1;32m--> 241\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mupdate_step(gradient, variable)\n",
      "File \u001b[1;32me:\\.scoop\\apps\\python310\\current\\lib\\site-packages\\keras\\src\\optimizers\\adam.py:204\u001b[0m, in \u001b[0;36mAdam.update_step\u001b[1;34m(self, gradient, variable)\u001b[0m\n\u001b[0;32m    202\u001b[0m     v_hat\u001b[39m.\u001b[39massign(tf\u001b[39m.\u001b[39mmaximum(v_hat, v))\n\u001b[0;32m    203\u001b[0m     v \u001b[39m=\u001b[39m v_hat\n\u001b[1;32m--> 204\u001b[0m variable\u001b[39m.\u001b[39massign_sub((m \u001b[39m*\u001b[39;49m alpha) \u001b[39m/\u001b[39m (tf\u001b[39m.\u001b[39msqrt(v) \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mepsilon))\n",
      "File \u001b[1;32me:\\.scoop\\apps\\python310\\current\\lib\\site-packages\\tensorflow\\python\\ops\\variables.py:1013\u001b[0m, in \u001b[0;36mVariable._OverloadOperator.<locals>._run_op\u001b[1;34m(a, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1011\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_run_op\u001b[39m(a, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m   1012\u001b[0m   \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m-> 1013\u001b[0m   \u001b[39mreturn\u001b[39;00m tensor_oper(a\u001b[39m.\u001b[39;49mvalue(), \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32me:\\.scoop\\apps\\python310\\current\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:632\u001b[0m, in \u001b[0;36mBaseResourceVariable.value\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    630\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_cached_value\n\u001b[0;32m    631\u001b[0m \u001b[39mwith\u001b[39;00m ops\u001b[39m.\u001b[39mcolocate_with(\u001b[39mNone\u001b[39;00m, ignore_existing\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m):\n\u001b[1;32m--> 632\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_read_variable_op()\n",
      "File \u001b[1;32me:\\.scoop\\apps\\python310\\current\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:793\u001b[0m, in \u001b[0;36mBaseResourceVariable._read_variable_op\u001b[1;34m(self, no_copy)\u001b[0m\n\u001b[0;32m    791\u001b[0m       result \u001b[39m=\u001b[39m read_and_set_handle(no_copy)\n\u001b[0;32m    792\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 793\u001b[0m   result \u001b[39m=\u001b[39m read_and_set_handle(no_copy)\n\u001b[0;32m    795\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m context\u001b[39m.\u001b[39mexecuting_eagerly():\n\u001b[0;32m    796\u001b[0m   \u001b[39m# Note that if a control flow context is active the input of the read op\u001b[39;00m\n\u001b[0;32m    797\u001b[0m   \u001b[39m# might not actually be the handle. This line bypasses it.\u001b[39;00m\n\u001b[0;32m    798\u001b[0m   record\u001b[39m.\u001b[39mrecord_operation(\n\u001b[0;32m    799\u001b[0m       \u001b[39m\"\u001b[39m\u001b[39mReadVariableOp\u001b[39m\u001b[39m\"\u001b[39m, [result], [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandle],\n\u001b[0;32m    800\u001b[0m       backward_function\u001b[39m=\u001b[39m\u001b[39mlambda\u001b[39;00m x: [x],\n\u001b[0;32m    801\u001b[0m       forward_function\u001b[39m=\u001b[39m\u001b[39mlambda\u001b[39;00m x: [x])\n",
      "File \u001b[1;32me:\\.scoop\\apps\\python310\\current\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:783\u001b[0m, in \u001b[0;36mBaseResourceVariable._read_variable_op.<locals>.read_and_set_handle\u001b[1;34m(no_copy)\u001b[0m\n\u001b[0;32m    781\u001b[0m \u001b[39mif\u001b[39;00m no_copy \u001b[39mand\u001b[39;00m forward_compat\u001b[39m.\u001b[39mforward_compatible(\u001b[39m2022\u001b[39m, \u001b[39m5\u001b[39m, \u001b[39m3\u001b[39m):\n\u001b[0;32m    782\u001b[0m   gen_resource_variable_ops\u001b[39m.\u001b[39mdisable_copy_on_read(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandle)\n\u001b[1;32m--> 783\u001b[0m result \u001b[39m=\u001b[39m gen_resource_variable_ops\u001b[39m.\u001b[39;49mread_variable_op(\n\u001b[0;32m    784\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mhandle, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dtype)\n\u001b[0;32m    785\u001b[0m _maybe_set_handle_data(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dtype, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandle, result)\n\u001b[0;32m    786\u001b[0m \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[1;32me:\\.scoop\\apps\\python310\\current\\lib\\site-packages\\tensorflow\\python\\ops\\gen_resource_variable_ops.py:589\u001b[0m, in \u001b[0;36mread_variable_op\u001b[1;34m(resource, dtype, name)\u001b[0m\n\u001b[0;32m    587\u001b[0m \u001b[39mif\u001b[39;00m tld\u001b[39m.\u001b[39mis_eager:\n\u001b[0;32m    588\u001b[0m   \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 589\u001b[0m     _result \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_FastPathExecute(\n\u001b[0;32m    590\u001b[0m       _ctx, \u001b[39m\"\u001b[39;49m\u001b[39mReadVariableOp\u001b[39;49m\u001b[39m\"\u001b[39;49m, name, resource, \u001b[39m\"\u001b[39;49m\u001b[39mdtype\u001b[39;49m\u001b[39m\"\u001b[39;49m, dtype)\n\u001b[0;32m    591\u001b[0m     \u001b[39mreturn\u001b[39;00m _result\n\u001b[0;32m    592\u001b[0m   \u001b[39mexcept\u001b[39;00m _core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# use the model\n",
    "dqnModel = DQN(\n",
    "    ACTIONNUM,\n",
    "    STATESHAPE,\n",
    "    EPSILON,\n",
    "    GAMMA,\n",
    "    BATCHSIZE,\n",
    "    POOLSIZE,\n",
    "    LEARNINGRATE,\n",
    "    env,\n",
    "    STARTCOUNT,\n",
    "    LEARNINGCUT,\n",
    "    UPDATETARGETCUT,\n",
    "    SAVEPATH,\n",
    "    SAVENAME,\n",
    ")\n",
    "\n",
    "for i in range(100):\n",
    "    reward, step, loss = dqnModel.Train()\n",
    "    print(\"round: %d, reward: %.2f step: %d loss: %.2f\" % (i + 1, reward, step, loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[48.0,\n",
       " 40.0,\n",
       " 9.0,\n",
       " 9.0,\n",
       " 11.0,\n",
       " 10.0,\n",
       " 14.0,\n",
       " 14.0,\n",
       " 10.0,\n",
       " 9.0,\n",
       " 13.0,\n",
       " 12.0,\n",
       " 9.0,\n",
       " 9.0,\n",
       " 13.0,\n",
       " 10.0,\n",
       " 13.0,\n",
       " 31.0,\n",
       " 10.0,\n",
       " 11.0,\n",
       " 8.0,\n",
       " 11.0,\n",
       " 12.0,\n",
       " 11.0,\n",
       " 10.0,\n",
       " 12.0,\n",
       " 18.0,\n",
       " 20.0,\n",
       " 9.0,\n",
       " 11.0,\n",
       " 9.0,\n",
       " 14.0,\n",
       " 11.0,\n",
       " 9.0,\n",
       " 10.0,\n",
       " 13.0,\n",
       " 10.0,\n",
       " 9.0,\n",
       " 9.0,\n",
       " 9.0,\n",
       " 8.0,\n",
       " 9.0,\n",
       " 11.0,\n",
       " 9.0,\n",
       " 13.0,\n",
       " 11.0,\n",
       " 12.0,\n",
       " 16.0,\n",
       " 25.0,\n",
       " 12.0,\n",
       " 11.0,\n",
       " 13.0,\n",
       " 11.0,\n",
       " 11.0,\n",
       " 12.0,\n",
       " 17.0,\n",
       " 12.0,\n",
       " 13.0,\n",
       " 11.0,\n",
       " 12.0,\n",
       " 12.0,\n",
       " 14.0,\n",
       " 14.0,\n",
       " 16.0,\n",
       " 20.0,\n",
       " 9.0,\n",
       " 11.0,\n",
       " 12.0,\n",
       " 10.0,\n",
       " 9.0,\n",
       " 9.0,\n",
       " 10.0,\n",
       " 28.0,\n",
       " 11.0,\n",
       " 14.0,\n",
       " 13.0,\n",
       " 11.0,\n",
       " 13.0,\n",
       " 12.0,\n",
       " 17.0,\n",
       " 12.0,\n",
       " 9.0,\n",
       " 13.0,\n",
       " 12.0,\n",
       " 11.0,\n",
       " 10.0,\n",
       " 12.0,\n",
       " 10.0,\n",
       " 8.0,\n",
       " 11.0,\n",
       " 9.0,\n",
       " 13.0,\n",
       " 14.0,\n",
       " 8.0,\n",
       " 9.0,\n",
       " 13.0,\n",
       " 18.0,\n",
       " 13.0,\n",
       " 40.0,\n",
       " 13.0,\n",
       " 9.0,\n",
       " 12.0,\n",
       " 9.0,\n",
       " 16.0,\n",
       " 10.0,\n",
       " 9.0,\n",
       " 9.0,\n",
       " 12.0,\n",
       " 12.0,\n",
       " 13.0,\n",
       " 9.0,\n",
       " 10.0,\n",
       " 13.0,\n",
       " 14.0,\n",
       " 9.0,\n",
       " 11.0,\n",
       " 11.0,\n",
       " 10.0,\n",
       " 11.0,\n",
       " 10.0,\n",
       " 11.0,\n",
       " 9.0,\n",
       " 11.0,\n",
       " 10.0,\n",
       " 10.0,\n",
       " 11.0,\n",
       " 14.0,\n",
       " 14.0,\n",
       " 8.0,\n",
       " 9.0,\n",
       " 10.0,\n",
       " 10.0,\n",
       " 10.0,\n",
       " 10.0,\n",
       " 13.0,\n",
       " 9.0,\n",
       " 10.0,\n",
       " 10.0,\n",
       " 12.0,\n",
       " 9.0,\n",
       " 10.0,\n",
       " 8.0,\n",
       " 11.0,\n",
       " 10.0,\n",
       " 10.0,\n",
       " 10.0,\n",
       " 10.0,\n",
       " 9.0,\n",
       " 10.0,\n",
       " 11.0,\n",
       " 17.0,\n",
       " 10.0,\n",
       " 11.0,\n",
       " 12.0,\n",
       " 12.0,\n",
       " 12.0,\n",
       " 12.0,\n",
       " 12.0,\n",
       " 11.0,\n",
       " 11.0,\n",
       " 12.0,\n",
       " 12.0,\n",
       " 14.0,\n",
       " 31.0,\n",
       " 11.0,\n",
       " 10.0,\n",
       " 18.0,\n",
       " 14.0,\n",
       " 16.0,\n",
       " 11.0,\n",
       " 14.0,\n",
       " 11.0,\n",
       " 14.0,\n",
       " 12.0,\n",
       " 10.0,\n",
       " 10.0,\n",
       " 11.0,\n",
       " 9.0,\n",
       " 9.0,\n",
       " 9.0,\n",
       " 8.0,\n",
       " 8.0,\n",
       " 10.0,\n",
       " 10.0,\n",
       " 10.0,\n",
       " 14.0,\n",
       " 11.0,\n",
       " 19.0,\n",
       " 12.0,\n",
       " 10.0,\n",
       " 17.0,\n",
       " 13.0,\n",
       " 12.0,\n",
       " 9.0,\n",
       " 10.0,\n",
       " 14.0,\n",
       " 13.0,\n",
       " 9.0,\n",
       " 14.0,\n",
       " 24.0,\n",
       " 24.0,\n",
       " 10.0,\n",
       " 18.0,\n",
       " 12.0,\n",
       " 17.0,\n",
       " 17.0,\n",
       " 8.0,\n",
       " 12.0,\n",
       " 8.0,\n",
       " 24.0,\n",
       " 11.0,\n",
       " 11.0,\n",
       " 11.0,\n",
       " 10.0,\n",
       " 14.0,\n",
       " 11.0,\n",
       " 11.0,\n",
       " 11.0,\n",
       " 9.0,\n",
       " 10.0,\n",
       " 12.0,\n",
       " 10.0,\n",
       " 12.0,\n",
       " 17.0,\n",
       " 37.0,\n",
       " 13.0,\n",
       " 44.0,\n",
       " 11.0,\n",
       " 13.0,\n",
       " 11.0,\n",
       " 11.0,\n",
       " 16.0,\n",
       " 10.0,\n",
       " 11.0,\n",
       " 10.0,\n",
       " 9.0,\n",
       " 19.0,\n",
       " 14.0,\n",
       " 37.0,\n",
       " 9.0,\n",
       " 29.0,\n",
       " 9.0,\n",
       " 10.0,\n",
       " 10.0,\n",
       " 9.0,\n",
       " 13.0,\n",
       " 10.0,\n",
       " 11.0,\n",
       " 9.0,\n",
       " 13.0,\n",
       " 9.0,\n",
       " 10.0,\n",
       " 13.0,\n",
       " 9.0,\n",
       " 12.0,\n",
       " 10.0,\n",
       " 10.0,\n",
       " 14.0,\n",
       " 17.0,\n",
       " 9.0,\n",
       " 16.0,\n",
       " 10.0,\n",
       " 11.0,\n",
       " 11.0,\n",
       " 10.0,\n",
       " 14.0,\n",
       " 9.0,\n",
       " 9.0,\n",
       " 15.0,\n",
       " 12.0,\n",
       " 10.0,\n",
       " 10.0,\n",
       " 11.0,\n",
       " 11.0,\n",
       " 13.0,\n",
       " 11.0,\n",
       " 14.0,\n",
       " 11.0,\n",
       " 10.0,\n",
       " 13.0,\n",
       " 10.0,\n",
       " 10.0,\n",
       " 11.0,\n",
       " 10.0,\n",
       " 10.0,\n",
       " 13.0,\n",
       " 12.0,\n",
       " 10.0,\n",
       " 13.0,\n",
       " 13.0,\n",
       " 11.0,\n",
       " 17.0,\n",
       " 11.0,\n",
       " 11.0,\n",
       " 11.0,\n",
       " 12.0,\n",
       " 11.0,\n",
       " 12.0,\n",
       " 11.0,\n",
       " 10.0,\n",
       " 9.0,\n",
       " 9.0,\n",
       " 10.0,\n",
       " 10.0,\n",
       " 11.0,\n",
       " 13.0,\n",
       " 12.0,\n",
       " 11.0,\n",
       " 9.0,\n",
       " 15.0,\n",
       " 10.0,\n",
       " 15.0,\n",
       " 10.0,\n",
       " 11.0,\n",
       " 10.0,\n",
       " 8.0,\n",
       " 10.0,\n",
       " 11.0,\n",
       " 10.0,\n",
       " 10.0,\n",
       " 10.0,\n",
       " 10.0,\n",
       " 9.0,\n",
       " 13.0,\n",
       " 13.0,\n",
       " 8.0,\n",
       " 18.0,\n",
       " 13.0,\n",
       " 11.0,\n",
       " 10.0,\n",
       " 13.0,\n",
       " 11.0,\n",
       " 12.0,\n",
       " 10.0,\n",
       " 14.0,\n",
       " 10.0,\n",
       " 9.0,\n",
       " 12.0,\n",
       " 12.0,\n",
       " 9.0,\n",
       " 14.0,\n",
       " 10.0,\n",
       " 12.0,\n",
       " 12.0,\n",
       " 9.0,\n",
       " 10.0,\n",
       " 12.0,\n",
       " 10.0,\n",
       " 12.0,\n",
       " 15.0,\n",
       " 14.0,\n",
       " 10.0,\n",
       " 11.0,\n",
       " 10.0,\n",
       " 11.0,\n",
       " 18.0,\n",
       " 10.0,\n",
       " 12.0,\n",
       " 14.0,\n",
       " 11.0,\n",
       " 10.0,\n",
       " 10.0,\n",
       " 9.0,\n",
       " 10.0,\n",
       " 10.0,\n",
       " 10.0,\n",
       " 10.0,\n",
       " 17.0,\n",
       " 8.0,\n",
       " 9.0,\n",
       " 10.0,\n",
       " 9.0,\n",
       " 11.0,\n",
       " 9.0,\n",
       " 12.0,\n",
       " 11.0,\n",
       " 10.0,\n",
       " 13.0,\n",
       " 11.0,\n",
       " 11.0,\n",
       " 10.0,\n",
       " 10.0,\n",
       " 9.0,\n",
       " 10.0,\n",
       " 12.0,\n",
       " 11.0,\n",
       " 13.0,\n",
       " 10.0,\n",
       " 9.0,\n",
       " 11.0,\n",
       " 14.0,\n",
       " 10.0,\n",
       " 11.0,\n",
       " 12.0,\n",
       " 11.0,\n",
       " 10.0,\n",
       " 9.0,\n",
       " 9.0,\n",
       " 10.0,\n",
       " 9.0,\n",
       " 9.0,\n",
       " 16.0,\n",
       " 13.0,\n",
       " 11.0,\n",
       " 10.0,\n",
       " 9.0,\n",
       " 13.0,\n",
       " 14.0,\n",
       " 10.0,\n",
       " 9.0,\n",
       " 10.0,\n",
       " 14.0,\n",
       " 26.0,\n",
       " 9.0,\n",
       " 10.0,\n",
       " 11.0,\n",
       " 13.0,\n",
       " 29.0,\n",
       " 13.0,\n",
       " 13.0,\n",
       " 10.0,\n",
       " 14.0,\n",
       " 9.0,\n",
       " 13.0,\n",
       " 12.0,\n",
       " 14.0,\n",
       " 10.0,\n",
       " 11.0,\n",
       " 12.0,\n",
       " 10.0,\n",
       " 9.0,\n",
       " 10.0,\n",
       " 10.0,\n",
       " 10.0,\n",
       " 12.0,\n",
       " 11.0,\n",
       " 12.0,\n",
       " 13.0,\n",
       " 11.0,\n",
       " 16.0,\n",
       " 10.0,\n",
       " 12.0,\n",
       " 10.0,\n",
       " 10.0,\n",
       " 13.0,\n",
       " 10.0,\n",
       " 12.0,\n",
       " 12.0,\n",
       " 12.0,\n",
       " 16.0,\n",
       " 8.0,\n",
       " 21.0,\n",
       " 12.0,\n",
       " 11.0,\n",
       " 8.0,\n",
       " 11.0,\n",
       " 9.0,\n",
       " 11.0,\n",
       " 9.0,\n",
       " 12.0,\n",
       " 9.0,\n",
       " 13.0,\n",
       " 10.0,\n",
       " 9.0,\n",
       " 11.0,\n",
       " 12.0,\n",
       " 11.0,\n",
       " 14.0,\n",
       " 11.0,\n",
       " 11.0,\n",
       " 11.0,\n",
       " 15.0,\n",
       " 10.0,\n",
       " 9.0,\n",
       " 13.0,\n",
       " 11.0,\n",
       " 9.0,\n",
       " 10.0,\n",
       " 10.0,\n",
       " 15.0,\n",
       " 10.0,\n",
       " 10.0,\n",
       " 10.0,\n",
       " 9.0,\n",
       " 14.0,\n",
       " 16.0,\n",
       " 11.0,\n",
       " 13.0,\n",
       " 15.0,\n",
       " 19.0,\n",
       " 9.0,\n",
       " 15.0,\n",
       " 10.0,\n",
       " 10.0,\n",
       " 9.0,\n",
       " 9.0,\n",
       " 9.0,\n",
       " 13.0,\n",
       " 9.0,\n",
       " 10.0,\n",
       " 9.0,\n",
       " 9.0,\n",
       " 11.0,\n",
       " 8.0,\n",
       " 14.0,\n",
       " 12.0,\n",
       " 12.0,\n",
       " 13.0,\n",
       " 11.0,\n",
       " 10.0,\n",
       " 12.0,\n",
       " 11.0,\n",
       " 9.0,\n",
       " 17.0,\n",
       " 11.0,\n",
       " 9.0,\n",
       " 14.0,\n",
       " 10.0,\n",
       " 9.0,\n",
       " 9.0,\n",
       " 9.0,\n",
       " 12.0,\n",
       " 16.0,\n",
       " 12.0,\n",
       " 10.0,\n",
       " 17.0,\n",
       " 11.0,\n",
       " 10.0,\n",
       " 9.0,\n",
       " 15.0,\n",
       " 12.0,\n",
       " 11.0,\n",
       " 12.0,\n",
       " 10.0,\n",
       " 15.0,\n",
       " 12.0,\n",
       " 13.0,\n",
       " 17.0,\n",
       " 10.0,\n",
       " 13.0,\n",
       " 9.0,\n",
       " 12.0,\n",
       " 12.0,\n",
       " 9.0,\n",
       " 12.0,\n",
       " 15.0,\n",
       " 8.0,\n",
       " 14.0,\n",
       " 14.0,\n",
       " 9.0,\n",
       " 22.0,\n",
       " 9.0,\n",
       " 10.0,\n",
       " 14.0,\n",
       " 10.0,\n",
       " 13.0,\n",
       " 21.0,\n",
       " 10.0,\n",
       " 13.0,\n",
       " 11.0,\n",
       " 16.0,\n",
       " 12.0,\n",
       " 12.0,\n",
       " 9.0,\n",
       " 15.0,\n",
       " 9.0,\n",
       " 10.0,\n",
       " 12.0,\n",
       " 10.0,\n",
       " 8.0,\n",
       " 11.0,\n",
       " 12.0,\n",
       " 13.0,\n",
       " 10.0,\n",
       " 14.0,\n",
       " 11.0,\n",
       " 10.0,\n",
       " 13.0,\n",
       " 10.0,\n",
       " 10.0,\n",
       " 10.0,\n",
       " 16.0,\n",
       " 9.0,\n",
       " 13.0,\n",
       " 12.0,\n",
       " 12.0,\n",
       " 11.0,\n",
       " 15.0,\n",
       " 9.0,\n",
       " 12.0,\n",
       " 8.0,\n",
       " 9.0,\n",
       " 11.0,\n",
       " 9.0,\n",
       " 11.0,\n",
       " 11.0,\n",
       " 15.0,\n",
       " 10.0,\n",
       " 10.0,\n",
       " 10.0,\n",
       " 16.0,\n",
       " 10.0,\n",
       " 11.0,\n",
       " 10.0,\n",
       " 13.0,\n",
       " 9.0,\n",
       " 10.0,\n",
       " 9.0,\n",
       " 17.0,\n",
       " 11.0,\n",
       " 10.0,\n",
       " 16.0,\n",
       " 8.0,\n",
       " 10.0,\n",
       " 9.0,\n",
       " 10.0,\n",
       " 8.0,\n",
       " 9.0,\n",
       " 10.0,\n",
       " 15.0,\n",
       " 11.0,\n",
       " 10.0,\n",
       " 12.0,\n",
       " 9.0,\n",
       " 16.0,\n",
       " 9.0,\n",
       " 10.0,\n",
       " 12.0,\n",
       " 13.0,\n",
       " 9.0,\n",
       " 11.0,\n",
       " 10.0,\n",
       " 9.0,\n",
       " 13.0,\n",
       " 9.0,\n",
       " 17.0,\n",
       " 14.0,\n",
       " 14.0,\n",
       " 12.0,\n",
       " 11.0,\n",
       " 8.0,\n",
       " 11.0,\n",
       " 9.0,\n",
       " 9.0,\n",
       " 11.0,\n",
       " 13.0,\n",
       " 12.0,\n",
       " 11.0,\n",
       " 10.0,\n",
       " 16.0,\n",
       " 11.0,\n",
       " 9.0,\n",
       " 9.0,\n",
       " 12.0,\n",
       " 10.0,\n",
       " 11.0,\n",
       " 9.0,\n",
       " 10.0,\n",
       " 10.0,\n",
       " 12.0,\n",
       " 11.0,\n",
       " 9.0,\n",
       " 9.0,\n",
       " 10.0,\n",
       " 9.0,\n",
       " 9.0,\n",
       " 14.0,\n",
       " 10.0,\n",
       " 11.0,\n",
       " 15.0,\n",
       " 12.0,\n",
       " 11.0,\n",
       " 13.0,\n",
       " 12.0,\n",
       " 12.0,\n",
       " 11.0,\n",
       " 10.0,\n",
       " 11.0,\n",
       " 13.0,\n",
       " 10.0,\n",
       " 9.0,\n",
       " 15.0,\n",
       " 9.0,\n",
       " 9.0,\n",
       " 11.0,\n",
       " 10.0,\n",
       " 8.0,\n",
       " 11.0,\n",
       " 13.0,\n",
       " 10.0,\n",
       " 13.0,\n",
       " 9.0,\n",
       " 9.0,\n",
       " 10.0,\n",
       " 8.0,\n",
       " 11.0,\n",
       " 10.0,\n",
       " 13.0,\n",
       " 10.0,\n",
       " 12.0,\n",
       " 8.0,\n",
       " 11.0,\n",
       " 11.0,\n",
       " 11.0,\n",
       " 13.0,\n",
       " 12.0,\n",
       " 13.0,\n",
       " 17.0,\n",
       " 9.0,\n",
       " 10.0,\n",
       " 14.0,\n",
       " 11.0,\n",
       " 9.0,\n",
       " 9.0,\n",
       " 10.0,\n",
       " 10.0,\n",
       " 15.0,\n",
       " 14.0,\n",
       " 13.0,\n",
       " 14.0,\n",
       " 15.0,\n",
       " 15.0,\n",
       " 11.0,\n",
       " 9.0,\n",
       " 10.0,\n",
       " 12.0,\n",
       " 18.0,\n",
       " 10.0,\n",
       " 11.0,\n",
       " 14.0,\n",
       " 11.0,\n",
       " 11.0,\n",
       " 11.0,\n",
       " 9.0,\n",
       " 8.0,\n",
       " 8.0,\n",
       " 10.0,\n",
       " 13.0,\n",
       " 14.0,\n",
       " 11.0,\n",
       " 13.0,\n",
       " 12.0,\n",
       " 11.0,\n",
       " 12.0,\n",
       " 11.0,\n",
       " 16.0,\n",
       " 13.0,\n",
       " 9.0,\n",
       " 10.0,\n",
       " 12.0,\n",
       " 13.0,\n",
       " 17.0,\n",
       " 15.0,\n",
       " 10.0,\n",
       " 10.0,\n",
       " 12.0,\n",
       " 8.0,\n",
       " 16.0,\n",
       " 12.0,\n",
       " 18.0,\n",
       " 10.0,\n",
       " 15.0,\n",
       " 9.0,\n",
       " 12.0,\n",
       " 9.0,\n",
       " 12.0,\n",
       " 11.0,\n",
       " 13.0,\n",
       " 8.0,\n",
       " 15.0,\n",
       " 13.0,\n",
       " 8.0,\n",
       " 9.0,\n",
       " 11.0,\n",
       " 12.0,\n",
       " 11.0,\n",
       " 20.0,\n",
       " 10.0,\n",
       " 10.0,\n",
       " 8.0,\n",
       " 8.0,\n",
       " 12.0,\n",
       " 9.0,\n",
       " 10.0,\n",
       " 10.0,\n",
       " 10.0,\n",
       " 11.0,\n",
       " 12.0,\n",
       " 10.0,\n",
       " 19.0,\n",
       " 10.0,\n",
       " 10.0,\n",
       " 9.0,\n",
       " 11.0,\n",
       " 9.0,\n",
       " 18.0,\n",
       " 15.0,\n",
       " 8.0,\n",
       " 12.0,\n",
       " 12.0,\n",
       " 18.0,\n",
       " 12.0,\n",
       " 9.0,\n",
       " 8.0,\n",
       " 10.0,\n",
       " 13.0,\n",
       " 11.0,\n",
       " 9.0,\n",
       " 18.0,\n",
       " 11.0,\n",
       " 18.0,\n",
       " 11.0,\n",
       " 12.0,\n",
       " 13.0,\n",
       " 13.0,\n",
       " 12.0,\n",
       " 18.0,\n",
       " 12.0,\n",
       " 10.0,\n",
       " 15.0,\n",
       " 10.0,\n",
       " 10.0,\n",
       " 9.0,\n",
       " 10.0,\n",
       " 10.0,\n",
       " 10.0,\n",
       " 12.0,\n",
       " 10.0,\n",
       " 17.0,\n",
       " 11.0,\n",
       " 10.0,\n",
       " 9.0,\n",
       " 10.0,\n",
       " 12.0,\n",
       " 10.0,\n",
       " 10.0,\n",
       " 9.0,\n",
       " 10.0,\n",
       " 11.0,\n",
       " 15.0,\n",
       " 11.0,\n",
       " 11.0,\n",
       " 9.0,\n",
       " 11.0,\n",
       " 10.0,\n",
       " 8.0,\n",
       " 10.0,\n",
       " 10.0,\n",
       " 8.0,\n",
       " 15.0,\n",
       " 11.0,\n",
       " 10.0,\n",
       " 15.0,\n",
       " 8.0,\n",
       " 13.0,\n",
       " 10.0,\n",
       " 12.0,\n",
       " 15.0,\n",
       " 9.0,\n",
       " 10.0,\n",
       " 11.0,\n",
       " 10.0,\n",
       " 9.0,\n",
       " 8.0,\n",
       " 14.0,\n",
       " 13.0,\n",
       " 9.0,\n",
       " 10.0,\n",
       " 10.0,\n",
       " 8.0,\n",
       " 9.0,\n",
       " 10.0,\n",
       " 11.0,\n",
       " 11.0,\n",
       " 12.0,\n",
       " 11.0,\n",
       " 9.0,\n",
       " 14.0,\n",
       " 10.0,\n",
       " 18.0,\n",
       " 11.0,\n",
       " 9.0,\n",
       " 11.0,\n",
       " 12.0,\n",
       " 13.0,\n",
       " 11.0,\n",
       " 11.0,\n",
       " 9.0,\n",
       " 9.0,\n",
       " 16.0,\n",
       " 11.0,\n",
       " 10.0,\n",
       " 15.0,\n",
       " 11.0,\n",
       " 11.0,\n",
       " 13.0,\n",
       " 8.0,\n",
       " 9.0,\n",
       " 9.0,\n",
       " 9.0,\n",
       " 10.0,\n",
       " 11.0,\n",
       " 14.0,\n",
       " 14.0,\n",
       " 12.0,\n",
       " 10.0,\n",
       " 10.0,\n",
       " 10.0,\n",
       " 11.0,\n",
       " 10.0,\n",
       " 15.0,\n",
       " 14.0,\n",
       " 10.0,\n",
       " 12.0,\n",
       " 12.0,\n",
       " 13.0,\n",
       " 10.0,\n",
       " 12.0,\n",
       " 10.0,\n",
       " 11.0,\n",
       " 10.0,\n",
       " 9.0,\n",
       " 11.0,\n",
       " 8.0,\n",
       " 12.0,\n",
       " 9.0,\n",
       " 10.0,\n",
       " 11.0,\n",
       " 10.0,\n",
       " 11.0,\n",
       " 8.0,\n",
       " 10.0,\n",
       " 8.0,\n",
       " 15.0,\n",
       " 11.0,\n",
       " 12.0,\n",
       " 12.0,\n",
       " 10.0,\n",
       " 13.0,\n",
       " 21.0,\n",
       " 14.0,\n",
       " 11.0,\n",
       " 12.0,\n",
       " 12.0,\n",
       " 12.0,\n",
       " 12.0,\n",
       " 9.0,\n",
       " 12.0,\n",
       " 14.0,\n",
       " 17.0,\n",
       " 11.0,\n",
       " 19.0,\n",
       " 18.0,\n",
       " 15.0,\n",
       " 9.0,\n",
       " 9.0,\n",
       " 13.0,\n",
       " 10.0,\n",
       " 15.0,\n",
       " 14.0,\n",
       " 9.0,\n",
       " 10.0,\n",
       " 10.0,\n",
       " 9.0,\n",
       " 10.0,\n",
       " 9.0,\n",
       " 9.0,\n",
       " 11.0,\n",
       " 11.0,\n",
       " 9.0,\n",
       " 12.0,\n",
       " 9.0,\n",
       " 13.0,\n",
       " 9.0,\n",
       " 13.0,\n",
       " 11.0,\n",
       " 10.0,\n",
       " 10.0,\n",
       " 10.0,\n",
       " 11.0,\n",
       " 13.0,\n",
       " 10.0,\n",
       " 10.0,\n",
       " 10.0,\n",
       " 10.0,\n",
       " 9.0,\n",
       " 12.0,\n",
       " 15.0,\n",
       " 11.0,\n",
       " 11.0,\n",
       " 12.0,\n",
       " 10.0,\n",
       " 11.0,\n",
       " 9.0,\n",
       " 10.0,\n",
       " 10.0,\n",
       " 11.0,\n",
       " 9.0,\n",
       " 8.0,\n",
       " 9.0]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dqnModel.rewardsStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32md:\\AI\\ReinforcementLearning\\DQN1.ipynb Cell 6\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/AI/ReinforcementLearning/DQN1.ipynb#W5sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import torch"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
