{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DQN\n",
    "import keras\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import gymnasium as gym\n",
    "import os\n",
    "import tensorflow as tf\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "use the dense to generate the action\n",
    "\"\"\"\n",
    "\n",
    "class Network(keras.Model):\n",
    "    def __init__(self, actionNum, stateShape, learningRate):\n",
    "        super(Network, self).__init__()\n",
    "\n",
    "        self.model = keras.Sequential(\n",
    "            [\n",
    "                keras.layers.Dense(32, input_shape=stateShape),\n",
    "                keras.layers.LeakyReLU(),\n",
    "                keras.layers.Dense(64),\n",
    "                keras.layers.LeakyReLU(),\n",
    "                keras.layers.Dense(actionNum),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.model.compile(\n",
    "            loss = 'mse',\n",
    "            optimizer = keras.optimizers.Adam(learningRate)\n",
    "        )\n",
    "\n",
    "        self.lossFunc = keras.losses.mean_squared_error\n",
    "        self.optimizer = keras.optimizers.Adam(learningRate)\n",
    "\n",
    "    def call(self, state):\n",
    "        return self.model(state, training=False)\n",
    "\n",
    "    # @tf.function\n",
    "    def Update(self, lastState, nextState, targetNetwork, reward, gamma):\n",
    "        with tf.GradientTape() as tape:\n",
    "            evaluateValue = tf.reduce_max(self.model(lastState, training=True).numpy(), axis = 1)\n",
    "            targetValue = targetNetwork(nextState, training=False)\n",
    "\n",
    "            targetValue = reward + gamma * np.max(targetValue.numpy(), axis=1)\n",
    "            print(evaluateValue.shape, targetValue.shape)\n",
    "            # //TODO 修改梯度下降法\n",
    "            loss = self.lossFunc(evaluateValue, targetValue)\n",
    "        gradients = tape.gradient(loss, self.model.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(gradients, self.model.trainable_variables))\n",
    "\n",
    "        return sum(loss)\n",
    "\n",
    "    def CopyVariable(self, network):\n",
    "        \"\"\"\n",
    "        param:\n",
    "            network:    the other network model\n",
    "                type:   Sequencial\n",
    "        func:\n",
    "            Copy the variable of other network model to self\n",
    "        \"\"\"\n",
    "        for selfLayer, otherLayer in zip(self.model.layers, network.layers):\n",
    "            selfLayer.set_weights(otherLayer.get_weights())\n",
    "\n",
    "\n",
    "class DQN:\n",
    "    def __init__(\n",
    "        self,\n",
    "        actionNum,\n",
    "        stateShape,\n",
    "        epsilon,\n",
    "        gamma,\n",
    "        batchSize,\n",
    "        poolSize,\n",
    "        learningRate,\n",
    "        environment,\n",
    "        startCount,\n",
    "        learningCut,\n",
    "        updateTargetCut,\n",
    "        savePath,\n",
    "        saveName,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        param:\n",
    "            actionNum:          the number of action\n",
    "            stateShape:         the shape of state\n",
    "            epsilon:            the parameter which is used in e-Greedy\n",
    "            gamma:              used in the loss of Q Max\n",
    "            batchSize:          the batch size\n",
    "            poolSize:           the size of memory pool\n",
    "            learningRate:       for optimize the network\n",
    "            environment:        the game which network will learn to play\n",
    "            startCount:         the step when network start to update\n",
    "            learningCut:        after one episode's step reach to the startCount, every learningCut we will update the parameter of network\n",
    "            updateTargetCut:  when reach this number, copy the weight of ChooseActionNetwork to the TargetNetwork\n",
    "\n",
    "            savePath:           the dir model will be saved\n",
    "            saveName:           the name model will be saved\n",
    "        \"\"\"\n",
    "\n",
    "        self.environment = environment\n",
    "\n",
    "        self.actionNum = actionNum\n",
    "        self.stateShape = stateShape\n",
    "        self.epsilon = epsilon\n",
    "        self.gamma = gamma\n",
    "        self.batchSize = batchSize\n",
    "        self.poolSize = poolSize\n",
    "\n",
    "        self.chooseActionNet = Network(actionNum, stateShape, learningRate)\n",
    "        self.targetNetwork = Network(actionNum, stateShape, learningRate)\n",
    "\n",
    "        self.memoryPool = {\n",
    "            \"state\": [],\n",
    "            \"action\": [],\n",
    "            \"reward\": [],\n",
    "            \"nextState\": [],\n",
    "        }\n",
    "\n",
    "        # self.memoryPool = []\n",
    "        self.memoryLength = 0\n",
    "\n",
    "        self.startCount = startCount\n",
    "        self.learningCut = learningCut\n",
    "        self.updateTargetCut = updateTargetCut\n",
    "\n",
    "        self.savePath = savePath\n",
    "        self.saveName = saveName\n",
    "\n",
    "        # to store the rewards\n",
    "        self.rewardsStore = []\n",
    "\n",
    "        # if not os.path.exists(savePath):\n",
    "        #     os.mkdir(savePath)\n",
    "\n",
    "    def ChooseAction(self, state):\n",
    "        if np.random.uniform() > self.epsilon:\n",
    "            return np.random.randint(self.actionNum)\n",
    "\n",
    "        predictValue = self.chooseActionNet.call(state)\n",
    "        return np.argmax(predictValue)\n",
    "\n",
    "    def StoreMemory(self, lastState, action, reward, nextState):\n",
    "        self.memoryPool[\"state\"].append(np.array(lastState))\n",
    "        self.memoryPool[\"action\"].append(np.array(action))\n",
    "        self.memoryPool[\"reward\"].append(np.array(reward))\n",
    "        self.memoryPool[\"nextState\"].append(np.array(nextState))\n",
    "\n",
    "        if self.memoryLength >= self.poolSize:\n",
    "            self.memoryPool[\"state\"].pop(0)\n",
    "            self.memoryPool[\"action\"].pop(0)\n",
    "            self.memoryPool[\"reward\"].pop(0)\n",
    "            self.memoryPool[\"nextState\"].pop(0)\n",
    "        else:\n",
    "            self.memoryLength += 1\n",
    "        # self.memoryPool.append(\n",
    "        #     (lastState, action, reward, nextState)\n",
    "        # )\n",
    "\n",
    "    def ChooseMemory(self):\n",
    "        \"\"\"\n",
    "        func:\n",
    "            Select a batchSize of observations from the memoryPool\n",
    "        return:\n",
    "            laststate\n",
    "            action\n",
    "            reward\n",
    "            nextState\n",
    "        \"\"\"\n",
    "\n",
    "        # 生成需要的 index\n",
    "        index = np.random.choice(self.memoryLength - 1, size = self.batchSize)\n",
    "        laststate = []\n",
    "        action = []\n",
    "        reward = []\n",
    "        nextState = []\n",
    "\n",
    "        for idx in index:\n",
    "            laststate.append(self.memoryPool[\"state\"][idx])\n",
    "            action.append(self.memoryPool[\"action\"][idx])\n",
    "            reward.append(self.memoryPool[\"reward\"][idx])\n",
    "            nextState.append(self.memoryPool[\"nextState\"][idx])\n",
    "\n",
    "        return (\n",
    "            np.array(laststate),\n",
    "            np.array(action),\n",
    "            np.array(reward),\n",
    "            np.array(nextState),\n",
    "        )\n",
    "\n",
    "    def UpdateNetwork(self):\n",
    "        \"\"\"\n",
    "        param:\n",
    "\n",
    "        func:\n",
    "            use the memory to make the batch to feed the network\n",
    "        \"\"\"\n",
    "\n",
    "        lastState, action, reward, nextState = self.ChooseMemory()\n",
    "\n",
    "        # history = self.chooseActionNet.model.fit(evaluation, target)\n",
    "        # loss = self.chooseActionNet.Update(\n",
    "        #     lastState, nextState, self.targetNetwork.model, reward, self.gamma\n",
    "        # )\n",
    "\n",
    "        targetValue = self.chooseActionNet.model.predict(lastState, verbose = 0)\n",
    "\n",
    "        target = self.targetNetwork.model.predict(nextState, verbose = 0)\n",
    "        targetValue[range(self.batchSize), action] = reward + self.gamma * np.max(target, axis = 1)\n",
    "        \n",
    "        history = self.chooseActionNet.model.fit(lastState, targetValue, verbose = 0, epochs = 5)\n",
    "\n",
    "        return sum(history.history['loss']) / float(self.batchSize)\n",
    "\n",
    "    def CopyNetwork(self):\n",
    "        \"\"\"\n",
    "        while copies the variable, this function will store the weight to the disk\n",
    "        \"\"\"\n",
    "\n",
    "        self.targetNetwork.CopyVariable(self.chooseActionNet.model)\n",
    "\n",
    "        # Save Model\n",
    "        self.targetNetwork.model.save_weights(self.savePath + self.saveName)\n",
    "\n",
    "    def Train(self):\n",
    "        isDone = False\n",
    "        lastState, _ = self.environment.reset()\n",
    "        learningCount = 0\n",
    "        afterCount = 0\n",
    "\n",
    "        rewardSum = 0\n",
    "        loss = 0\n",
    "        while not isDone:\n",
    "            action = self.ChooseAction(np.array([lastState]))\n",
    "\n",
    "            nextState, reward, isDone, _, _ = self.environment.step(action)\n",
    "\n",
    "            self.StoreMemory(lastState, action, reward, nextState)\n",
    "\n",
    "            if learningCount > self.startCount:\n",
    "                if afterCount % self.learningCut == 0:\n",
    "                    loss += self.UpdateNetwork()\n",
    "\n",
    "                if afterCount % self.updateTargetCut == 0:\n",
    "                    self.CopyNetwork()\n",
    "\n",
    "                afterCount += 1\n",
    "\n",
    "            lastState = nextState\n",
    "            learningCount += 1\n",
    "            rewardSum += reward\n",
    "\n",
    "        self.rewardsStore.append(rewardSum)\n",
    "        return rewardSum, learningCount, loss / learningCount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4,)\n"
     ]
    }
   ],
   "source": [
    "# define the super parameter\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "ACTIONNUM = env.action_space.n\n",
    "STATESHAPE = env.observation_space.shape\n",
    "EPSILON = 0.7\n",
    "GAMMA = 0.8\n",
    "STARTCOUNT = 10\n",
    "LEARNINGCUT = 3\n",
    "UPDATETARGETCUT = 10\n",
    "BATCHSIZE = 16\n",
    "POOLSIZE = 200\n",
    "LEARNINGRATE = 0.3\n",
    "SAVEPATH = \"../SavedModel/RL/DQN/\"\n",
    "SAVENAME = \"DQN1weight.h5\"\n",
    "EPOCHS = 100\n",
    "\n",
    "print(STATESHAPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "round: 1, reward: 11.00 step: 11 loss: 0.00\n",
      "round: 2, reward: 10.00 step: 10 loss: 0.00\n",
      "round: 3, reward: 11.00 step: 11 loss: 0.00\n",
      "round: 4, reward: 10.00 step: 10 loss: 0.00\n",
      "round: 5, reward: 10.00 step: 10 loss: 0.00\n",
      "round: 6, reward: 17.00 step: 17 loss: 16.07\n",
      "round: 7, reward: 11.00 step: 11 loss: 0.00\n",
      "round: 8, reward: 12.00 step: 12 loss: 3.55\n",
      "round: 9, reward: 23.00 step: 23 loss: 3.35\n",
      "round: 10, reward: 9.00 step: 9 loss: 0.00\n",
      "round: 11, reward: 10.00 step: 10 loss: 0.00\n",
      "round: 12, reward: 12.00 step: 12 loss: 1.92\n",
      "round: 13, reward: 11.00 step: 11 loss: 0.00\n",
      "round: 14, reward: 17.00 step: 17 loss: 1.99\n",
      "round: 15, reward: 11.00 step: 11 loss: 0.00\n",
      "round: 16, reward: 10.00 step: 10 loss: 0.00\n",
      "round: 17, reward: 9.00 step: 9 loss: 0.00\n",
      "round: 18, reward: 12.00 step: 12 loss: 2.18\n",
      "round: 19, reward: 10.00 step: 10 loss: 0.00\n",
      "round: 20, reward: 10.00 step: 10 loss: 0.00\n",
      "round: 21, reward: 17.00 step: 17 loss: 0.56\n",
      "round: 22, reward: 13.00 step: 13 loss: 10.44\n",
      "round: 23, reward: 10.00 step: 10 loss: 0.00\n",
      "round: 24, reward: 11.00 step: 11 loss: 0.00\n",
      "round: 25, reward: 11.00 step: 11 loss: 0.00\n",
      "round: 26, reward: 12.00 step: 12 loss: 2.25\n",
      "round: 27, reward: 14.00 step: 14 loss: 0.73\n",
      "round: 28, reward: 13.00 step: 13 loss: 0.29\n",
      "round: 29, reward: 11.00 step: 11 loss: 0.00\n",
      "round: 30, reward: 11.00 step: 11 loss: 0.00\n",
      "round: 31, reward: 9.00 step: 9 loss: 0.00\n",
      "round: 32, reward: 13.00 step: 13 loss: 0.21\n",
      "round: 33, reward: 9.00 step: 9 loss: 0.00\n",
      "round: 34, reward: 10.00 step: 10 loss: 0.00\n",
      "round: 35, reward: 17.00 step: 17 loss: 0.13\n",
      "round: 36, reward: 15.00 step: 15 loss: 0.01\n",
      "round: 37, reward: 10.00 step: 10 loss: 0.00\n",
      "round: 38, reward: 15.00 step: 15 loss: 0.02\n",
      "round: 39, reward: 16.00 step: 16 loss: 0.01\n",
      "round: 40, reward: 11.00 step: 11 loss: 0.00\n",
      "round: 41, reward: 11.00 step: 11 loss: 0.00\n",
      "round: 42, reward: 13.00 step: 13 loss: 0.03\n",
      "round: 43, reward: 11.00 step: 11 loss: 0.00\n",
      "round: 44, reward: 12.00 step: 12 loss: 0.03\n",
      "round: 45, reward: 13.00 step: 13 loss: 0.03\n",
      "round: 46, reward: 13.00 step: 13 loss: 0.04\n",
      "round: 47, reward: 13.00 step: 13 loss: 0.04\n",
      "round: 48, reward: 10.00 step: 10 loss: 0.00\n",
      "round: 49, reward: 10.00 step: 10 loss: 0.00\n",
      "round: 50, reward: 13.00 step: 13 loss: 0.04\n",
      "round: 51, reward: 10.00 step: 10 loss: 0.00\n",
      "round: 52, reward: 12.00 step: 12 loss: 0.02\n",
      "round: 53, reward: 13.00 step: 13 loss: 0.01\n",
      "round: 54, reward: 12.00 step: 12 loss: 0.02\n",
      "round: 55, reward: 12.00 step: 12 loss: 0.01\n",
      "round: 56, reward: 13.00 step: 13 loss: 0.02\n",
      "round: 57, reward: 11.00 step: 11 loss: 0.00\n",
      "round: 58, reward: 11.00 step: 11 loss: 0.00\n",
      "round: 59, reward: 10.00 step: 10 loss: 0.00\n",
      "round: 60, reward: 12.00 step: 12 loss: 0.03\n",
      "round: 61, reward: 12.00 step: 12 loss: 0.03\n",
      "round: 62, reward: 14.00 step: 14 loss: 0.01\n",
      "round: 63, reward: 10.00 step: 10 loss: 0.00\n",
      "round: 64, reward: 13.00 step: 13 loss: 0.00\n",
      "round: 65, reward: 12.00 step: 12 loss: 0.00\n",
      "round: 66, reward: 11.00 step: 11 loss: 0.00\n",
      "round: 67, reward: 15.00 step: 15 loss: 0.00\n",
      "round: 68, reward: 11.00 step: 11 loss: 0.00\n",
      "round: 69, reward: 11.00 step: 11 loss: 0.00\n",
      "round: 70, reward: 10.00 step: 10 loss: 0.00\n",
      "round: 71, reward: 10.00 step: 10 loss: 0.00\n",
      "round: 72, reward: 10.00 step: 10 loss: 0.00\n",
      "round: 73, reward: 17.00 step: 17 loss: 0.01\n",
      "round: 74, reward: 14.00 step: 14 loss: 0.00\n",
      "round: 75, reward: 11.00 step: 11 loss: 0.00\n",
      "round: 76, reward: 12.00 step: 12 loss: 0.01\n",
      "round: 77, reward: 11.00 step: 11 loss: 0.00\n",
      "round: 78, reward: 12.00 step: 12 loss: 0.01\n",
      "round: 79, reward: 15.00 step: 15 loss: 0.00\n",
      "round: 80, reward: 11.00 step: 11 loss: 0.00\n",
      "round: 81, reward: 15.00 step: 15 loss: 0.01\n",
      "round: 82, reward: 8.00 step: 8 loss: 0.00\n",
      "round: 83, reward: 9.00 step: 9 loss: 0.00\n",
      "round: 84, reward: 9.00 step: 9 loss: 0.00\n",
      "round: 85, reward: 9.00 step: 9 loss: 0.00\n",
      "round: 86, reward: 9.00 step: 9 loss: 0.00\n",
      "round: 87, reward: 11.00 step: 11 loss: 0.00\n",
      "round: 88, reward: 9.00 step: 9 loss: 0.00\n",
      "round: 89, reward: 13.00 step: 13 loss: 0.01\n",
      "round: 90, reward: 12.00 step: 12 loss: 0.00\n",
      "round: 91, reward: 12.00 step: 12 loss: 0.01\n",
      "round: 92, reward: 11.00 step: 11 loss: 0.00\n",
      "round: 93, reward: 11.00 step: 11 loss: 0.00\n",
      "round: 94, reward: 13.00 step: 13 loss: 0.01\n",
      "round: 95, reward: 15.00 step: 15 loss: 0.01\n",
      "round: 96, reward: 9.00 step: 9 loss: 0.00\n",
      "round: 97, reward: 8.00 step: 8 loss: 0.00\n",
      "round: 98, reward: 10.00 step: 10 loss: 0.00\n",
      "round: 99, reward: 13.00 step: 13 loss: 0.02\n",
      "round: 100, reward: 14.00 step: 14 loss: 0.01\n",
      "round: 101, reward: 9.00 step: 9 loss: 0.00\n",
      "round: 102, reward: 14.00 step: 14 loss: 0.00\n",
      "round: 103, reward: 9.00 step: 9 loss: 0.00\n",
      "round: 104, reward: 10.00 step: 10 loss: 0.00\n",
      "round: 105, reward: 8.00 step: 8 loss: 0.00\n",
      "round: 106, reward: 9.00 step: 9 loss: 0.00\n",
      "round: 107, reward: 8.00 step: 8 loss: 0.00\n",
      "round: 108, reward: 10.00 step: 10 loss: 0.00\n",
      "round: 109, reward: 12.00 step: 12 loss: 0.01\n",
      "round: 110, reward: 10.00 step: 10 loss: 0.00\n",
      "round: 111, reward: 9.00 step: 9 loss: 0.00\n",
      "round: 112, reward: 9.00 step: 9 loss: 0.00\n",
      "round: 113, reward: 16.00 step: 16 loss: 0.00\n",
      "round: 114, reward: 11.00 step: 11 loss: 0.00\n",
      "round: 115, reward: 10.00 step: 10 loss: 0.00\n",
      "round: 116, reward: 10.00 step: 10 loss: 0.00\n",
      "round: 117, reward: 11.00 step: 11 loss: 0.00\n",
      "round: 118, reward: 8.00 step: 8 loss: 0.00\n",
      "round: 119, reward: 9.00 step: 9 loss: 0.00\n",
      "round: 120, reward: 11.00 step: 11 loss: 0.00\n",
      "round: 121, reward: 11.00 step: 11 loss: 0.00\n",
      "round: 122, reward: 11.00 step: 11 loss: 0.00\n",
      "round: 123, reward: 10.00 step: 10 loss: 0.00\n",
      "round: 124, reward: 9.00 step: 9 loss: 0.00\n",
      "round: 125, reward: 11.00 step: 11 loss: 0.00\n",
      "round: 126, reward: 10.00 step: 10 loss: 0.00\n",
      "round: 127, reward: 9.00 step: 9 loss: 0.00\n",
      "round: 128, reward: 10.00 step: 10 loss: 0.00\n",
      "round: 129, reward: 14.00 step: 14 loss: 0.01\n",
      "round: 130, reward: 10.00 step: 10 loss: 0.00\n",
      "round: 131, reward: 9.00 step: 9 loss: 0.00\n",
      "round: 132, reward: 9.00 step: 9 loss: 0.00\n",
      "round: 133, reward: 11.00 step: 11 loss: 0.00\n",
      "round: 134, reward: 9.00 step: 9 loss: 0.00\n",
      "round: 135, reward: 9.00 step: 9 loss: 0.00\n",
      "round: 136, reward: 10.00 step: 10 loss: 0.00\n",
      "round: 137, reward: 11.00 step: 11 loss: 0.00\n",
      "round: 138, reward: 8.00 step: 8 loss: 0.00\n",
      "round: 139, reward: 11.00 step: 11 loss: 0.00\n",
      "round: 140, reward: 11.00 step: 11 loss: 0.00\n",
      "round: 141, reward: 8.00 step: 8 loss: 0.00\n",
      "round: 142, reward: 13.00 step: 13 loss: 0.00\n",
      "round: 143, reward: 10.00 step: 10 loss: 0.00\n",
      "round: 144, reward: 13.00 step: 13 loss: 0.00\n",
      "round: 145, reward: 11.00 step: 11 loss: 0.00\n",
      "round: 146, reward: 9.00 step: 9 loss: 0.00\n",
      "round: 147, reward: 11.00 step: 11 loss: 0.00\n",
      "round: 148, reward: 13.00 step: 13 loss: 0.01\n",
      "round: 149, reward: 11.00 step: 11 loss: 0.00\n",
      "round: 150, reward: 10.00 step: 10 loss: 0.00\n",
      "round: 151, reward: 19.00 step: 19 loss: 0.01\n",
      "round: 152, reward: 11.00 step: 11 loss: 0.00\n",
      "round: 153, reward: 9.00 step: 9 loss: 0.00\n",
      "round: 154, reward: 10.00 step: 10 loss: 0.00\n",
      "round: 155, reward: 18.00 step: 18 loss: 0.01\n",
      "round: 156, reward: 10.00 step: 10 loss: 0.00\n",
      "round: 157, reward: 10.00 step: 10 loss: 0.00\n",
      "round: 158, reward: 10.00 step: 10 loss: 0.00\n",
      "round: 159, reward: 12.00 step: 12 loss: 0.00\n",
      "round: 160, reward: 10.00 step: 10 loss: 0.00\n",
      "round: 161, reward: 11.00 step: 11 loss: 0.00\n",
      "round: 162, reward: 9.00 step: 9 loss: 0.00\n",
      "round: 163, reward: 10.00 step: 10 loss: 0.00\n",
      "round: 164, reward: 10.00 step: 10 loss: 0.00\n",
      "round: 165, reward: 9.00 step: 9 loss: 0.00\n",
      "round: 166, reward: 10.00 step: 10 loss: 0.00\n",
      "round: 167, reward: 9.00 step: 9 loss: 0.00\n",
      "round: 168, reward: 8.00 step: 8 loss: 0.00\n",
      "round: 169, reward: 10.00 step: 10 loss: 0.00\n",
      "round: 170, reward: 9.00 step: 9 loss: 0.00\n",
      "round: 171, reward: 8.00 step: 8 loss: 0.00\n",
      "round: 172, reward: 9.00 step: 9 loss: 0.00\n",
      "round: 173, reward: 13.00 step: 13 loss: 0.00\n",
      "round: 174, reward: 10.00 step: 10 loss: 0.00\n",
      "round: 175, reward: 14.00 step: 14 loss: 0.00\n",
      "round: 176, reward: 9.00 step: 9 loss: 0.00\n",
      "round: 177, reward: 8.00 step: 8 loss: 0.00\n",
      "round: 178, reward: 13.00 step: 13 loss: 0.00\n",
      "round: 179, reward: 10.00 step: 10 loss: 0.00\n",
      "round: 180, reward: 10.00 step: 10 loss: 0.00\n",
      "round: 181, reward: 10.00 step: 10 loss: 0.00\n",
      "round: 182, reward: 12.00 step: 12 loss: 0.00\n",
      "round: 183, reward: 14.00 step: 14 loss: 0.00\n",
      "round: 184, reward: 11.00 step: 11 loss: 0.00\n",
      "round: 185, reward: 10.00 step: 10 loss: 0.00\n",
      "round: 186, reward: 10.00 step: 10 loss: 0.00\n",
      "round: 187, reward: 11.00 step: 11 loss: 0.00\n",
      "round: 188, reward: 10.00 step: 10 loss: 0.00\n",
      "round: 189, reward: 13.00 step: 13 loss: 0.00\n",
      "round: 190, reward: 10.00 step: 10 loss: 0.00\n",
      "round: 191, reward: 10.00 step: 10 loss: 0.00\n",
      "round: 192, reward: 9.00 step: 9 loss: 0.00\n",
      "round: 193, reward: 11.00 step: 11 loss: 0.00\n",
      "round: 194, reward: 9.00 step: 9 loss: 0.00\n",
      "round: 195, reward: 11.00 step: 11 loss: 0.00\n",
      "round: 196, reward: 10.00 step: 10 loss: 0.00\n",
      "round: 197, reward: 10.00 step: 10 loss: 0.00\n",
      "round: 198, reward: 10.00 step: 10 loss: 0.00\n",
      "round: 199, reward: 11.00 step: 11 loss: 0.00\n",
      "round: 200, reward: 11.00 step: 11 loss: 0.00\n",
      "round: 201, reward: 12.00 step: 12 loss: 0.01\n",
      "round: 202, reward: 9.00 step: 9 loss: 0.00\n",
      "round: 203, reward: 15.00 step: 15 loss: 0.00\n",
      "round: 204, reward: 11.00 step: 11 loss: 0.00\n",
      "round: 205, reward: 10.00 step: 10 loss: 0.00\n",
      "round: 206, reward: 9.00 step: 9 loss: 0.00\n",
      "round: 207, reward: 29.00 step: 29 loss: 0.00\n",
      "round: 208, reward: 13.00 step: 13 loss: 0.00\n",
      "round: 209, reward: 11.00 step: 11 loss: 0.00\n",
      "round: 210, reward: 26.00 step: 26 loss: 0.00\n",
      "round: 211, reward: 10.00 step: 10 loss: 0.00\n",
      "round: 212, reward: 11.00 step: 11 loss: 0.00\n",
      "round: 213, reward: 13.00 step: 13 loss: 0.00\n",
      "round: 214, reward: 14.00 step: 14 loss: 0.00\n",
      "round: 215, reward: 9.00 step: 9 loss: 0.00\n",
      "round: 216, reward: 13.00 step: 13 loss: 0.00\n",
      "round: 217, reward: 19.00 step: 19 loss: 0.00\n",
      "round: 218, reward: 15.00 step: 15 loss: 0.00\n",
      "round: 219, reward: 11.00 step: 11 loss: 0.00\n",
      "round: 220, reward: 11.00 step: 11 loss: 0.00\n",
      "round: 221, reward: 9.00 step: 9 loss: 0.00\n",
      "round: 222, reward: 11.00 step: 11 loss: 0.00\n",
      "round: 223, reward: 9.00 step: 9 loss: 0.00\n",
      "round: 224, reward: 9.00 step: 9 loss: 0.00\n",
      "round: 225, reward: 8.00 step: 8 loss: 0.00\n",
      "round: 226, reward: 10.00 step: 10 loss: 0.00\n",
      "round: 227, reward: 11.00 step: 11 loss: 0.00\n",
      "round: 228, reward: 10.00 step: 10 loss: 0.00\n",
      "round: 229, reward: 9.00 step: 9 loss: 0.00\n",
      "round: 230, reward: 10.00 step: 10 loss: 0.00\n",
      "round: 231, reward: 12.00 step: 12 loss: 0.00\n",
      "round: 232, reward: 10.00 step: 10 loss: 0.00\n",
      "round: 233, reward: 28.00 step: 28 loss: 0.00\n",
      "round: 234, reward: 12.00 step: 12 loss: 0.00\n",
      "round: 235, reward: 11.00 step: 11 loss: 0.00\n",
      "round: 236, reward: 9.00 step: 9 loss: 0.00\n",
      "round: 237, reward: 10.00 step: 10 loss: 0.00\n",
      "round: 238, reward: 10.00 step: 10 loss: 0.00\n",
      "round: 239, reward: 12.00 step: 12 loss: 0.00\n",
      "round: 240, reward: 11.00 step: 11 loss: 0.00\n",
      "round: 241, reward: 10.00 step: 10 loss: 0.00\n",
      "round: 242, reward: 11.00 step: 11 loss: 0.00\n",
      "round: 243, reward: 10.00 step: 10 loss: 0.00\n",
      "round: 244, reward: 10.00 step: 10 loss: 0.00\n",
      "round: 245, reward: 11.00 step: 11 loss: 0.00\n",
      "round: 246, reward: 15.00 step: 15 loss: 0.00\n",
      "round: 247, reward: 12.00 step: 12 loss: 0.00\n",
      "round: 248, reward: 11.00 step: 11 loss: 0.00\n",
      "round: 249, reward: 11.00 step: 11 loss: 0.00\n",
      "round: 250, reward: 12.00 step: 12 loss: 0.00\n",
      "round: 251, reward: 13.00 step: 13 loss: 0.00\n",
      "round: 252, reward: 13.00 step: 13 loss: 0.00\n",
      "round: 253, reward: 13.00 step: 13 loss: 0.00\n",
      "round: 254, reward: 9.00 step: 9 loss: 0.00\n",
      "round: 255, reward: 38.00 step: 38 loss: 0.00\n",
      "round: 256, reward: 9.00 step: 9 loss: 0.00\n",
      "round: 257, reward: 11.00 step: 11 loss: 0.00\n",
      "round: 258, reward: 9.00 step: 9 loss: 0.00\n",
      "round: 259, reward: 13.00 step: 13 loss: 0.00\n",
      "round: 260, reward: 9.00 step: 9 loss: 0.00\n",
      "round: 261, reward: 10.00 step: 10 loss: 0.00\n",
      "round: 262, reward: 11.00 step: 11 loss: 0.00\n",
      "round: 263, reward: 9.00 step: 9 loss: 0.00\n",
      "round: 264, reward: 10.00 step: 10 loss: 0.00\n",
      "round: 265, reward: 9.00 step: 9 loss: 0.00\n",
      "round: 266, reward: 10.00 step: 10 loss: 0.00\n",
      "round: 267, reward: 11.00 step: 11 loss: 0.00\n",
      "round: 268, reward: 11.00 step: 11 loss: 0.00\n",
      "round: 269, reward: 16.00 step: 16 loss: 0.00\n",
      "round: 270, reward: 11.00 step: 11 loss: 0.00\n",
      "round: 271, reward: 15.00 step: 15 loss: 0.00\n",
      "round: 272, reward: 13.00 step: 13 loss: 0.00\n",
      "round: 273, reward: 10.00 step: 10 loss: 0.00\n",
      "round: 274, reward: 11.00 step: 11 loss: 0.00\n",
      "round: 275, reward: 12.00 step: 12 loss: 0.00\n",
      "round: 276, reward: 11.00 step: 11 loss: 0.00\n",
      "round: 277, reward: 10.00 step: 10 loss: 0.00\n",
      "round: 278, reward: 10.00 step: 10 loss: 0.00\n",
      "round: 279, reward: 14.00 step: 14 loss: 0.00\n",
      "round: 280, reward: 11.00 step: 11 loss: 0.00\n",
      "round: 281, reward: 10.00 step: 10 loss: 0.00\n",
      "round: 282, reward: 11.00 step: 11 loss: 0.00\n",
      "round: 283, reward: 9.00 step: 9 loss: 0.00\n",
      "round: 284, reward: 10.00 step: 10 loss: 0.00\n",
      "round: 285, reward: 10.00 step: 10 loss: 0.00\n",
      "round: 286, reward: 10.00 step: 10 loss: 0.00\n",
      "round: 287, reward: 10.00 step: 10 loss: 0.00\n",
      "round: 288, reward: 9.00 step: 9 loss: 0.00\n",
      "round: 289, reward: 10.00 step: 10 loss: 0.00\n",
      "round: 290, reward: 9.00 step: 9 loss: 0.00\n",
      "round: 291, reward: 10.00 step: 10 loss: 0.00\n",
      "round: 292, reward: 10.00 step: 10 loss: 0.00\n",
      "round: 293, reward: 11.00 step: 11 loss: 0.00\n",
      "round: 294, reward: 11.00 step: 11 loss: 0.00\n",
      "round: 295, reward: 10.00 step: 10 loss: 0.00\n",
      "round: 296, reward: 14.00 step: 14 loss: 0.00\n",
      "round: 297, reward: 9.00 step: 9 loss: 0.00\n",
      "round: 298, reward: 9.00 step: 9 loss: 0.00\n",
      "round: 299, reward: 8.00 step: 8 loss: 0.00\n",
      "round: 300, reward: 8.00 step: 8 loss: 0.00\n",
      "round: 301, reward: 8.00 step: 8 loss: 0.00\n",
      "round: 302, reward: 12.00 step: 12 loss: 0.00\n",
      "round: 303, reward: 20.00 step: 20 loss: 0.00\n",
      "round: 304, reward: 11.00 step: 11 loss: 0.00\n",
      "round: 305, reward: 10.00 step: 10 loss: 0.00\n",
      "round: 306, reward: 9.00 step: 9 loss: 0.00\n",
      "round: 307, reward: 9.00 step: 9 loss: 0.00\n",
      "round: 308, reward: 9.00 step: 9 loss: 0.00\n",
      "round: 309, reward: 10.00 step: 10 loss: 0.00\n",
      "round: 310, reward: 14.00 step: 14 loss: 0.00\n",
      "round: 311, reward: 9.00 step: 9 loss: 0.00\n",
      "round: 312, reward: 9.00 step: 9 loss: 0.00\n",
      "round: 313, reward: 13.00 step: 13 loss: 0.00\n",
      "round: 314, reward: 22.00 step: 22 loss: 0.00\n",
      "round: 315, reward: 10.00 step: 10 loss: 0.00\n",
      "round: 316, reward: 9.00 step: 9 loss: 0.00\n",
      "round: 317, reward: 16.00 step: 16 loss: 0.00\n",
      "round: 318, reward: 9.00 step: 9 loss: 0.00\n",
      "round: 319, reward: 9.00 step: 9 loss: 0.00\n",
      "round: 320, reward: 17.00 step: 17 loss: 0.00\n",
      "round: 321, reward: 8.00 step: 8 loss: 0.00\n",
      "round: 322, reward: 15.00 step: 15 loss: 0.00\n",
      "round: 323, reward: 13.00 step: 13 loss: 0.00\n",
      "round: 324, reward: 10.00 step: 10 loss: 0.00\n",
      "round: 325, reward: 9.00 step: 9 loss: 0.00\n",
      "round: 326, reward: 9.00 step: 9 loss: 0.00\n",
      "round: 327, reward: 14.00 step: 14 loss: 0.00\n",
      "round: 328, reward: 11.00 step: 11 loss: 0.00\n",
      "round: 329, reward: 13.00 step: 13 loss: 0.00\n",
      "round: 330, reward: 11.00 step: 11 loss: 0.00\n",
      "round: 331, reward: 10.00 step: 10 loss: 0.00\n",
      "round: 332, reward: 10.00 step: 10 loss: 0.00\n",
      "round: 333, reward: 11.00 step: 11 loss: 0.00\n",
      "round: 334, reward: 9.00 step: 9 loss: 0.00\n",
      "round: 335, reward: 12.00 step: 12 loss: 0.00\n",
      "round: 336, reward: 8.00 step: 8 loss: 0.00\n",
      "round: 337, reward: 12.00 step: 12 loss: 0.00\n",
      "round: 338, reward: 11.00 step: 11 loss: 0.00\n",
      "round: 339, reward: 13.00 step: 13 loss: 0.00\n",
      "round: 340, reward: 14.00 step: 14 loss: 0.00\n",
      "round: 341, reward: 9.00 step: 9 loss: 0.00\n",
      "round: 342, reward: 12.00 step: 12 loss: 0.00\n",
      "round: 343, reward: 11.00 step: 11 loss: 0.00\n",
      "round: 344, reward: 9.00 step: 9 loss: 0.00\n",
      "round: 345, reward: 12.00 step: 12 loss: 0.00\n",
      "round: 346, reward: 11.00 step: 11 loss: 0.00\n",
      "round: 347, reward: 10.00 step: 10 loss: 0.00\n",
      "round: 348, reward: 10.00 step: 10 loss: 0.00\n",
      "round: 349, reward: 9.00 step: 9 loss: 0.00\n",
      "round: 350, reward: 9.00 step: 9 loss: 0.00\n",
      "round: 351, reward: 12.00 step: 12 loss: 0.00\n",
      "round: 352, reward: 12.00 step: 12 loss: 0.00\n",
      "round: 353, reward: 8.00 step: 8 loss: 0.00\n",
      "round: 354, reward: 14.00 step: 14 loss: 0.00\n",
      "round: 355, reward: 10.00 step: 10 loss: 0.00\n",
      "round: 356, reward: 19.00 step: 19 loss: 0.00\n",
      "round: 357, reward: 10.00 step: 10 loss: 0.00\n",
      "round: 358, reward: 11.00 step: 11 loss: 0.00\n",
      "round: 359, reward: 10.00 step: 10 loss: 0.00\n",
      "round: 360, reward: 13.00 step: 13 loss: 0.00\n",
      "round: 361, reward: 10.00 step: 10 loss: 0.00\n",
      "round: 362, reward: 13.00 step: 13 loss: 0.00\n",
      "round: 363, reward: 10.00 step: 10 loss: 0.00\n",
      "round: 364, reward: 17.00 step: 17 loss: 0.00\n",
      "round: 365, reward: 10.00 step: 10 loss: 0.00\n",
      "round: 366, reward: 13.00 step: 13 loss: 0.00\n",
      "round: 367, reward: 9.00 step: 9 loss: 0.00\n",
      "round: 368, reward: 10.00 step: 10 loss: 0.00\n",
      "round: 369, reward: 10.00 step: 10 loss: 0.00\n",
      "round: 370, reward: 19.00 step: 19 loss: 0.00\n",
      "round: 371, reward: 12.00 step: 12 loss: 0.00\n",
      "round: 372, reward: 11.00 step: 11 loss: 0.00\n",
      "round: 373, reward: 9.00 step: 9 loss: 0.00\n",
      "round: 374, reward: 9.00 step: 9 loss: 0.00\n",
      "round: 375, reward: 12.00 step: 12 loss: 0.00\n",
      "round: 376, reward: 9.00 step: 9 loss: 0.00\n",
      "round: 377, reward: 15.00 step: 15 loss: 0.00\n",
      "round: 378, reward: 9.00 step: 9 loss: 0.00\n",
      "round: 379, reward: 10.00 step: 10 loss: 0.00\n",
      "round: 380, reward: 12.00 step: 12 loss: 0.00\n",
      "round: 381, reward: 10.00 step: 10 loss: 0.00\n",
      "round: 382, reward: 12.00 step: 12 loss: 0.00\n",
      "round: 383, reward: 10.00 step: 10 loss: 0.00\n",
      "round: 384, reward: 9.00 step: 9 loss: 0.00\n",
      "round: 385, reward: 10.00 step: 10 loss: 0.00\n",
      "round: 386, reward: 11.00 step: 11 loss: 0.00\n",
      "round: 387, reward: 11.00 step: 11 loss: 0.00\n",
      "round: 388, reward: 10.00 step: 10 loss: 0.00\n",
      "round: 389, reward: 12.00 step: 12 loss: 0.00\n",
      "round: 390, reward: 9.00 step: 9 loss: 0.00\n",
      "round: 391, reward: 10.00 step: 10 loss: 0.00\n",
      "round: 392, reward: 11.00 step: 11 loss: 0.00\n",
      "round: 393, reward: 11.00 step: 11 loss: 0.00\n",
      "round: 394, reward: 10.00 step: 10 loss: 0.00\n",
      "round: 395, reward: 11.00 step: 11 loss: 0.00\n",
      "round: 396, reward: 10.00 step: 10 loss: 0.00\n",
      "round: 397, reward: 11.00 step: 11 loss: 0.00\n",
      "round: 398, reward: 10.00 step: 10 loss: 0.00\n",
      "round: 399, reward: 9.00 step: 9 loss: 0.00\n",
      "round: 400, reward: 13.00 step: 13 loss: 0.00\n",
      "round: 401, reward: 14.00 step: 14 loss: 0.00\n",
      "round: 402, reward: 12.00 step: 12 loss: 0.00\n",
      "round: 403, reward: 12.00 step: 12 loss: 0.00\n",
      "round: 404, reward: 10.00 step: 10 loss: 0.00\n",
      "round: 405, reward: 12.00 step: 12 loss: 0.00\n",
      "round: 406, reward: 12.00 step: 12 loss: 0.00\n",
      "round: 407, reward: 12.00 step: 12 loss: 0.00\n",
      "round: 408, reward: 16.00 step: 16 loss: 0.00\n",
      "round: 409, reward: 9.00 step: 9 loss: 0.00\n",
      "round: 410, reward: 12.00 step: 12 loss: 0.00\n",
      "round: 411, reward: 11.00 step: 11 loss: 0.00\n",
      "round: 412, reward: 10.00 step: 10 loss: 0.00\n",
      "round: 413, reward: 9.00 step: 9 loss: 0.00\n",
      "round: 414, reward: 11.00 step: 11 loss: 0.00\n",
      "round: 415, reward: 9.00 step: 9 loss: 0.00\n",
      "round: 416, reward: 9.00 step: 9 loss: 0.00\n",
      "round: 417, reward: 15.00 step: 15 loss: 0.00\n",
      "round: 418, reward: 11.00 step: 11 loss: 0.00\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32md:\\AI\\ReinforcementLearning\\DQN1.ipynb Cell 4\u001b[0m line \u001b[0;36m1\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/AI/ReinforcementLearning/DQN1.ipynb#W3sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m dqnModel \u001b[39m=\u001b[39m DQN(\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/AI/ReinforcementLearning/DQN1.ipynb#W3sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     ACTIONNUM,\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/AI/ReinforcementLearning/DQN1.ipynb#W3sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     STATESHAPE,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/AI/ReinforcementLearning/DQN1.ipynb#W3sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m     SAVENAME,\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/AI/ReinforcementLearning/DQN1.ipynb#W3sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m )\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/AI/ReinforcementLearning/DQN1.ipynb#W3sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1000\u001b[39m):\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/AI/ReinforcementLearning/DQN1.ipynb#W3sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m     reward, step, loss \u001b[39m=\u001b[39m dqnModel\u001b[39m.\u001b[39;49mTrain()\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/AI/ReinforcementLearning/DQN1.ipynb#W3sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mround: \u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m, reward: \u001b[39m\u001b[39m%.2f\u001b[39;00m\u001b[39m step: \u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m loss: \u001b[39m\u001b[39m%.2f\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m (i \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m, reward, step, loss))\n",
      "\u001b[1;32md:\\AI\\ReinforcementLearning\\DQN1.ipynb Cell 4\u001b[0m line \u001b[0;36m2\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/AI/ReinforcementLearning/DQN1.ipynb#W3sZmlsZQ%3D%3D?line=230'>231</a>\u001b[0m \u001b[39mif\u001b[39;00m learningCount \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstartCount:\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/AI/ReinforcementLearning/DQN1.ipynb#W3sZmlsZQ%3D%3D?line=231'>232</a>\u001b[0m     \u001b[39mif\u001b[39;00m afterCount \u001b[39m%\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlearningCut \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m--> <a href='vscode-notebook-cell:/d%3A/AI/ReinforcementLearning/DQN1.ipynb#W3sZmlsZQ%3D%3D?line=232'>233</a>\u001b[0m         loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mUpdateNetwork()\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/AI/ReinforcementLearning/DQN1.ipynb#W3sZmlsZQ%3D%3D?line=234'>235</a>\u001b[0m     \u001b[39mif\u001b[39;00m afterCount \u001b[39m%\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mupdateTargetCut \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/AI/ReinforcementLearning/DQN1.ipynb#W3sZmlsZQ%3D%3D?line=235'>236</a>\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mCopyNetwork()\n",
      "\u001b[1;32md:\\AI\\ReinforcementLearning\\DQN1.ipynb Cell 4\u001b[0m line \u001b[0;36m2\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/AI/ReinforcementLearning/DQN1.ipynb#W3sZmlsZQ%3D%3D?line=198'>199</a>\u001b[0m target \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtargetNetwork\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mpredict(nextState, verbose \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m)\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/AI/ReinforcementLearning/DQN1.ipynb#W3sZmlsZQ%3D%3D?line=199'>200</a>\u001b[0m targetValue[\u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatchSize), action] \u001b[39m=\u001b[39m reward \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgamma \u001b[39m*\u001b[39m np\u001b[39m.\u001b[39mmax(target, axis \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m)\n\u001b[1;32m--> <a href='vscode-notebook-cell:/d%3A/AI/ReinforcementLearning/DQN1.ipynb#W3sZmlsZQ%3D%3D?line=201'>202</a>\u001b[0m history \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mchooseActionNet\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49mfit(lastState, targetValue, verbose \u001b[39m=\u001b[39;49m \u001b[39m0\u001b[39;49m, epochs \u001b[39m=\u001b[39;49m \u001b[39m5\u001b[39;49m)\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/AI/ReinforcementLearning/DQN1.ipynb#W3sZmlsZQ%3D%3D?line=203'>204</a>\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msum\u001b[39m(history\u001b[39m.\u001b[39mhistory[\u001b[39m'\u001b[39m\u001b[39mloss\u001b[39m\u001b[39m'\u001b[39m]) \u001b[39m/\u001b[39m \u001b[39mfloat\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatchSize)\n",
      "File \u001b[1;32me:\\.scoop\\apps\\python310\\current\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32me:\\.scoop\\apps\\python310\\current\\lib\\site-packages\\keras\\src\\engine\\training.py:1770\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1763\u001b[0m (\n\u001b[0;32m   1764\u001b[0m     data_handler\u001b[39m.\u001b[39m_initial_epoch,\n\u001b[0;32m   1765\u001b[0m     data_handler\u001b[39m.\u001b[39m_initial_step,\n\u001b[0;32m   1766\u001b[0m ) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_load_initial_counters_from_ckpt(\n\u001b[0;32m   1767\u001b[0m     steps_per_epoch_inferred, initial_epoch\n\u001b[0;32m   1768\u001b[0m )\n\u001b[0;32m   1769\u001b[0m logs \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m-> 1770\u001b[0m \u001b[39mfor\u001b[39;00m epoch, iterator \u001b[39min\u001b[39;00m data_handler\u001b[39m.\u001b[39menumerate_epochs():\n\u001b[0;32m   1771\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreset_metrics()\n\u001b[0;32m   1772\u001b[0m     callbacks\u001b[39m.\u001b[39mon_epoch_begin(epoch)\n",
      "File \u001b[1;32me:\\.scoop\\apps\\python310\\current\\lib\\site-packages\\keras\\src\\engine\\data_adapter.py:1341\u001b[0m, in \u001b[0;36mDataHandler.enumerate_epochs\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1339\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Yields `(epoch, tf.data.Iterator)`.\"\"\"\u001b[39;00m\n\u001b[0;32m   1340\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_truncate_execution_to_epoch():\n\u001b[1;32m-> 1341\u001b[0m     data_iterator \u001b[39m=\u001b[39m \u001b[39miter\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset)\n\u001b[0;32m   1342\u001b[0m     \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_initial_epoch, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_epochs):\n\u001b[0;32m   1343\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_insufficient_data:  \u001b[39m# Set by `catch_stop_iteration`.\u001b[39;00m\n",
      "File \u001b[1;32me:\\.scoop\\apps\\python310\\current\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py:496\u001b[0m, in \u001b[0;36mDatasetV2.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    494\u001b[0m \u001b[39mif\u001b[39;00m context\u001b[39m.\u001b[39mexecuting_eagerly() \u001b[39mor\u001b[39;00m ops\u001b[39m.\u001b[39minside_function():\n\u001b[0;32m    495\u001b[0m   \u001b[39mwith\u001b[39;00m ops\u001b[39m.\u001b[39mcolocate_with(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_variant_tensor):\n\u001b[1;32m--> 496\u001b[0m     \u001b[39mreturn\u001b[39;00m iterator_ops\u001b[39m.\u001b[39;49mOwnedIterator(\u001b[39mself\u001b[39;49m)\n\u001b[0;32m    497\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    498\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m`tf.data.Dataset` only supports Python-style \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    499\u001b[0m                      \u001b[39m\"\u001b[39m\u001b[39miteration in eager mode or within tf.function.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32me:\\.scoop\\apps\\python310\\current\\lib\\site-packages\\tensorflow\\python\\data\\ops\\iterator_ops.py:705\u001b[0m, in \u001b[0;36mOwnedIterator.__init__\u001b[1;34m(self, dataset, components, element_spec)\u001b[0m\n\u001b[0;32m    701\u001b[0m   \u001b[39mif\u001b[39;00m (components \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m element_spec \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m):\n\u001b[0;32m    702\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    703\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mWhen `dataset` is provided, `element_spec` and `components` must \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    704\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mnot be specified.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 705\u001b[0m   \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_create_iterator(dataset)\n\u001b[0;32m    707\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_next_call_count \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n",
      "File \u001b[1;32me:\\.scoop\\apps\\python310\\current\\lib\\site-packages\\tensorflow\\python\\data\\ops\\iterator_ops.py:744\u001b[0m, in \u001b[0;36mOwnedIterator._create_iterator\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    741\u001b[0m   \u001b[39massert\u001b[39;00m \u001b[39mlen\u001b[39m(fulltype\u001b[39m.\u001b[39margs[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39margs[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39margs) \u001b[39m==\u001b[39m \u001b[39mlen\u001b[39m(\n\u001b[0;32m    742\u001b[0m       \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_flat_output_types)\n\u001b[0;32m    743\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iterator_resource\u001b[39m.\u001b[39mop\u001b[39m.\u001b[39mexperimental_set_type(fulltype)\n\u001b[1;32m--> 744\u001b[0m gen_dataset_ops\u001b[39m.\u001b[39;49mmake_iterator(ds_variant, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_iterator_resource)\n",
      "File \u001b[1;32me:\\.scoop\\apps\\python310\\current\\lib\\site-packages\\tensorflow\\python\\ops\\gen_dataset_ops.py:3451\u001b[0m, in \u001b[0;36mmake_iterator\u001b[1;34m(dataset, iterator, name)\u001b[0m\n\u001b[0;32m   3449\u001b[0m \u001b[39mif\u001b[39;00m tld\u001b[39m.\u001b[39mis_eager:\n\u001b[0;32m   3450\u001b[0m   \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 3451\u001b[0m     _result \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_FastPathExecute(\n\u001b[0;32m   3452\u001b[0m       _ctx, \u001b[39m\"\u001b[39;49m\u001b[39mMakeIterator\u001b[39;49m\u001b[39m\"\u001b[39;49m, name, dataset, iterator)\n\u001b[0;32m   3453\u001b[0m     \u001b[39mreturn\u001b[39;00m _result\n\u001b[0;32m   3454\u001b[0m   \u001b[39mexcept\u001b[39;00m _core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# use the model\n",
    "dqnModel = DQN(\n",
    "    ACTIONNUM,\n",
    "    STATESHAPE,\n",
    "    EPSILON,\n",
    "    GAMMA,\n",
    "    BATCHSIZE,\n",
    "    POOLSIZE,\n",
    "    LEARNINGRATE,\n",
    "    env,\n",
    "    STARTCOUNT,\n",
    "    LEARNINGCUT,\n",
    "    UPDATETARGETCUT,\n",
    "    SAVEPATH,\n",
    "    SAVENAME,\n",
    ")\n",
    "\n",
    "for i in range(1000):\n",
    "    reward, step, loss = dqnModel.Train()\n",
    "    print(\"round: %d, reward: %.2f step: %d loss: %.2f\" % (i + 1, reward, step, loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[48.0,\n",
       " 40.0,\n",
       " 9.0,\n",
       " 9.0,\n",
       " 11.0,\n",
       " 10.0,\n",
       " 14.0,\n",
       " 14.0,\n",
       " 10.0,\n",
       " 9.0,\n",
       " 13.0,\n",
       " 12.0,\n",
       " 9.0,\n",
       " 9.0,\n",
       " 13.0,\n",
       " 10.0,\n",
       " 13.0,\n",
       " 31.0,\n",
       " 10.0,\n",
       " 11.0,\n",
       " 8.0,\n",
       " 11.0,\n",
       " 12.0,\n",
       " 11.0,\n",
       " 10.0,\n",
       " 12.0,\n",
       " 18.0,\n",
       " 20.0,\n",
       " 9.0,\n",
       " 11.0,\n",
       " 9.0,\n",
       " 14.0,\n",
       " 11.0,\n",
       " 9.0,\n",
       " 10.0,\n",
       " 13.0,\n",
       " 10.0,\n",
       " 9.0,\n",
       " 9.0,\n",
       " 9.0,\n",
       " 8.0,\n",
       " 9.0,\n",
       " 11.0,\n",
       " 9.0,\n",
       " 13.0,\n",
       " 11.0,\n",
       " 12.0,\n",
       " 16.0,\n",
       " 25.0,\n",
       " 12.0,\n",
       " 11.0,\n",
       " 13.0,\n",
       " 11.0,\n",
       " 11.0,\n",
       " 12.0,\n",
       " 17.0,\n",
       " 12.0,\n",
       " 13.0,\n",
       " 11.0,\n",
       " 12.0,\n",
       " 12.0,\n",
       " 14.0,\n",
       " 14.0,\n",
       " 16.0,\n",
       " 20.0,\n",
       " 9.0,\n",
       " 11.0,\n",
       " 12.0,\n",
       " 10.0,\n",
       " 9.0,\n",
       " 9.0,\n",
       " 10.0,\n",
       " 28.0,\n",
       " 11.0,\n",
       " 14.0,\n",
       " 13.0,\n",
       " 11.0,\n",
       " 13.0,\n",
       " 12.0,\n",
       " 17.0,\n",
       " 12.0,\n",
       " 9.0,\n",
       " 13.0,\n",
       " 12.0,\n",
       " 11.0,\n",
       " 10.0,\n",
       " 12.0,\n",
       " 10.0,\n",
       " 8.0,\n",
       " 11.0,\n",
       " 9.0,\n",
       " 13.0,\n",
       " 14.0,\n",
       " 8.0,\n",
       " 9.0,\n",
       " 13.0,\n",
       " 18.0,\n",
       " 13.0,\n",
       " 40.0,\n",
       " 13.0,\n",
       " 9.0,\n",
       " 12.0,\n",
       " 9.0,\n",
       " 16.0,\n",
       " 10.0,\n",
       " 9.0,\n",
       " 9.0,\n",
       " 12.0,\n",
       " 12.0,\n",
       " 13.0,\n",
       " 9.0,\n",
       " 10.0,\n",
       " 13.0,\n",
       " 14.0,\n",
       " 9.0,\n",
       " 11.0,\n",
       " 11.0,\n",
       " 10.0,\n",
       " 11.0,\n",
       " 10.0,\n",
       " 11.0,\n",
       " 9.0,\n",
       " 11.0,\n",
       " 10.0,\n",
       " 10.0,\n",
       " 11.0,\n",
       " 14.0,\n",
       " 14.0,\n",
       " 8.0,\n",
       " 9.0,\n",
       " 10.0,\n",
       " 10.0,\n",
       " 10.0,\n",
       " 10.0,\n",
       " 13.0,\n",
       " 9.0,\n",
       " 10.0,\n",
       " 10.0,\n",
       " 12.0,\n",
       " 9.0,\n",
       " 10.0,\n",
       " 8.0,\n",
       " 11.0,\n",
       " 10.0,\n",
       " 10.0,\n",
       " 10.0,\n",
       " 10.0,\n",
       " 9.0,\n",
       " 10.0,\n",
       " 11.0,\n",
       " 17.0,\n",
       " 10.0,\n",
       " 11.0,\n",
       " 12.0,\n",
       " 12.0,\n",
       " 12.0,\n",
       " 12.0,\n",
       " 12.0,\n",
       " 11.0,\n",
       " 11.0,\n",
       " 12.0,\n",
       " 12.0,\n",
       " 14.0,\n",
       " 31.0,\n",
       " 11.0,\n",
       " 10.0,\n",
       " 18.0,\n",
       " 14.0,\n",
       " 16.0,\n",
       " 11.0,\n",
       " 14.0,\n",
       " 11.0,\n",
       " 14.0,\n",
       " 12.0,\n",
       " 10.0,\n",
       " 10.0,\n",
       " 11.0,\n",
       " 9.0,\n",
       " 9.0,\n",
       " 9.0,\n",
       " 8.0,\n",
       " 8.0,\n",
       " 10.0,\n",
       " 10.0,\n",
       " 10.0,\n",
       " 14.0,\n",
       " 11.0,\n",
       " 19.0,\n",
       " 12.0,\n",
       " 10.0,\n",
       " 17.0,\n",
       " 13.0,\n",
       " 12.0,\n",
       " 9.0,\n",
       " 10.0,\n",
       " 14.0,\n",
       " 13.0,\n",
       " 9.0,\n",
       " 14.0,\n",
       " 24.0,\n",
       " 24.0,\n",
       " 10.0,\n",
       " 18.0,\n",
       " 12.0,\n",
       " 17.0,\n",
       " 17.0,\n",
       " 8.0,\n",
       " 12.0,\n",
       " 8.0,\n",
       " 24.0,\n",
       " 11.0,\n",
       " 11.0,\n",
       " 11.0,\n",
       " 10.0,\n",
       " 14.0,\n",
       " 11.0,\n",
       " 11.0,\n",
       " 11.0,\n",
       " 9.0,\n",
       " 10.0,\n",
       " 12.0,\n",
       " 10.0,\n",
       " 12.0,\n",
       " 17.0,\n",
       " 37.0,\n",
       " 13.0,\n",
       " 44.0,\n",
       " 11.0,\n",
       " 13.0,\n",
       " 11.0,\n",
       " 11.0,\n",
       " 16.0,\n",
       " 10.0,\n",
       " 11.0,\n",
       " 10.0,\n",
       " 9.0,\n",
       " 19.0,\n",
       " 14.0,\n",
       " 37.0,\n",
       " 9.0,\n",
       " 29.0,\n",
       " 9.0,\n",
       " 10.0,\n",
       " 10.0,\n",
       " 9.0,\n",
       " 13.0,\n",
       " 10.0,\n",
       " 11.0,\n",
       " 9.0,\n",
       " 13.0,\n",
       " 9.0,\n",
       " 10.0,\n",
       " 13.0,\n",
       " 9.0,\n",
       " 12.0,\n",
       " 10.0,\n",
       " 10.0,\n",
       " 14.0,\n",
       " 17.0,\n",
       " 9.0,\n",
       " 16.0,\n",
       " 10.0,\n",
       " 11.0,\n",
       " 11.0,\n",
       " 10.0,\n",
       " 14.0,\n",
       " 9.0,\n",
       " 9.0,\n",
       " 15.0,\n",
       " 12.0,\n",
       " 10.0,\n",
       " 10.0,\n",
       " 11.0,\n",
       " 11.0,\n",
       " 13.0,\n",
       " 11.0,\n",
       " 14.0,\n",
       " 11.0,\n",
       " 10.0,\n",
       " 13.0,\n",
       " 10.0,\n",
       " 10.0,\n",
       " 11.0,\n",
       " 10.0,\n",
       " 10.0,\n",
       " 13.0,\n",
       " 12.0,\n",
       " 10.0,\n",
       " 13.0,\n",
       " 13.0,\n",
       " 11.0,\n",
       " 17.0,\n",
       " 11.0,\n",
       " 11.0,\n",
       " 11.0,\n",
       " 12.0,\n",
       " 11.0,\n",
       " 12.0,\n",
       " 11.0,\n",
       " 10.0,\n",
       " 9.0,\n",
       " 9.0,\n",
       " 10.0,\n",
       " 10.0,\n",
       " 11.0,\n",
       " 13.0,\n",
       " 12.0,\n",
       " 11.0,\n",
       " 9.0,\n",
       " 15.0,\n",
       " 10.0,\n",
       " 15.0,\n",
       " 10.0,\n",
       " 11.0,\n",
       " 10.0,\n",
       " 8.0,\n",
       " 10.0,\n",
       " 11.0,\n",
       " 10.0,\n",
       " 10.0,\n",
       " 10.0,\n",
       " 10.0,\n",
       " 9.0,\n",
       " 13.0,\n",
       " 13.0,\n",
       " 8.0,\n",
       " 18.0,\n",
       " 13.0,\n",
       " 11.0,\n",
       " 10.0,\n",
       " 13.0,\n",
       " 11.0,\n",
       " 12.0,\n",
       " 10.0,\n",
       " 14.0,\n",
       " 10.0,\n",
       " 9.0,\n",
       " 12.0,\n",
       " 12.0,\n",
       " 9.0,\n",
       " 14.0,\n",
       " 10.0,\n",
       " 12.0,\n",
       " 12.0,\n",
       " 9.0,\n",
       " 10.0,\n",
       " 12.0,\n",
       " 10.0,\n",
       " 12.0,\n",
       " 15.0,\n",
       " 14.0,\n",
       " 10.0,\n",
       " 11.0,\n",
       " 10.0,\n",
       " 11.0,\n",
       " 18.0,\n",
       " 10.0,\n",
       " 12.0,\n",
       " 14.0,\n",
       " 11.0,\n",
       " 10.0,\n",
       " 10.0,\n",
       " 9.0,\n",
       " 10.0,\n",
       " 10.0,\n",
       " 10.0,\n",
       " 10.0,\n",
       " 17.0,\n",
       " 8.0,\n",
       " 9.0,\n",
       " 10.0,\n",
       " 9.0,\n",
       " 11.0,\n",
       " 9.0,\n",
       " 12.0,\n",
       " 11.0,\n",
       " 10.0,\n",
       " 13.0,\n",
       " 11.0,\n",
       " 11.0,\n",
       " 10.0,\n",
       " 10.0,\n",
       " 9.0,\n",
       " 10.0,\n",
       " 12.0,\n",
       " 11.0,\n",
       " 13.0,\n",
       " 10.0,\n",
       " 9.0,\n",
       " 11.0,\n",
       " 14.0,\n",
       " 10.0,\n",
       " 11.0,\n",
       " 12.0,\n",
       " 11.0,\n",
       " 10.0,\n",
       " 9.0,\n",
       " 9.0,\n",
       " 10.0,\n",
       " 9.0,\n",
       " 9.0,\n",
       " 16.0,\n",
       " 13.0,\n",
       " 11.0,\n",
       " 10.0,\n",
       " 9.0,\n",
       " 13.0,\n",
       " 14.0,\n",
       " 10.0,\n",
       " 9.0,\n",
       " 10.0,\n",
       " 14.0,\n",
       " 26.0,\n",
       " 9.0,\n",
       " 10.0,\n",
       " 11.0,\n",
       " 13.0,\n",
       " 29.0,\n",
       " 13.0,\n",
       " 13.0,\n",
       " 10.0,\n",
       " 14.0,\n",
       " 9.0,\n",
       " 13.0,\n",
       " 12.0,\n",
       " 14.0,\n",
       " 10.0,\n",
       " 11.0,\n",
       " 12.0,\n",
       " 10.0,\n",
       " 9.0,\n",
       " 10.0,\n",
       " 10.0,\n",
       " 10.0,\n",
       " 12.0,\n",
       " 11.0,\n",
       " 12.0,\n",
       " 13.0,\n",
       " 11.0,\n",
       " 16.0,\n",
       " 10.0,\n",
       " 12.0,\n",
       " 10.0,\n",
       " 10.0,\n",
       " 13.0,\n",
       " 10.0,\n",
       " 12.0,\n",
       " 12.0,\n",
       " 12.0,\n",
       " 16.0,\n",
       " 8.0,\n",
       " 21.0,\n",
       " 12.0,\n",
       " 11.0,\n",
       " 8.0,\n",
       " 11.0,\n",
       " 9.0,\n",
       " 11.0,\n",
       " 9.0,\n",
       " 12.0,\n",
       " 9.0,\n",
       " 13.0,\n",
       " 10.0,\n",
       " 9.0,\n",
       " 11.0,\n",
       " 12.0,\n",
       " 11.0,\n",
       " 14.0,\n",
       " 11.0,\n",
       " 11.0,\n",
       " 11.0,\n",
       " 15.0,\n",
       " 10.0,\n",
       " 9.0,\n",
       " 13.0,\n",
       " 11.0,\n",
       " 9.0,\n",
       " 10.0,\n",
       " 10.0,\n",
       " 15.0,\n",
       " 10.0,\n",
       " 10.0,\n",
       " 10.0,\n",
       " 9.0,\n",
       " 14.0,\n",
       " 16.0,\n",
       " 11.0,\n",
       " 13.0,\n",
       " 15.0,\n",
       " 19.0,\n",
       " 9.0,\n",
       " 15.0,\n",
       " 10.0,\n",
       " 10.0,\n",
       " 9.0,\n",
       " 9.0,\n",
       " 9.0,\n",
       " 13.0,\n",
       " 9.0,\n",
       " 10.0,\n",
       " 9.0,\n",
       " 9.0,\n",
       " 11.0,\n",
       " 8.0,\n",
       " 14.0,\n",
       " 12.0,\n",
       " 12.0,\n",
       " 13.0,\n",
       " 11.0,\n",
       " 10.0,\n",
       " 12.0,\n",
       " 11.0,\n",
       " 9.0,\n",
       " 17.0,\n",
       " 11.0,\n",
       " 9.0,\n",
       " 14.0,\n",
       " 10.0,\n",
       " 9.0,\n",
       " 9.0,\n",
       " 9.0,\n",
       " 12.0,\n",
       " 16.0,\n",
       " 12.0,\n",
       " 10.0,\n",
       " 17.0,\n",
       " 11.0,\n",
       " 10.0,\n",
       " 9.0,\n",
       " 15.0,\n",
       " 12.0,\n",
       " 11.0,\n",
       " 12.0,\n",
       " 10.0,\n",
       " 15.0,\n",
       " 12.0,\n",
       " 13.0,\n",
       " 17.0,\n",
       " 10.0,\n",
       " 13.0,\n",
       " 9.0,\n",
       " 12.0,\n",
       " 12.0,\n",
       " 9.0,\n",
       " 12.0,\n",
       " 15.0,\n",
       " 8.0,\n",
       " 14.0,\n",
       " 14.0,\n",
       " 9.0,\n",
       " 22.0,\n",
       " 9.0,\n",
       " 10.0,\n",
       " 14.0,\n",
       " 10.0,\n",
       " 13.0,\n",
       " 21.0,\n",
       " 10.0,\n",
       " 13.0,\n",
       " 11.0,\n",
       " 16.0,\n",
       " 12.0,\n",
       " 12.0,\n",
       " 9.0,\n",
       " 15.0,\n",
       " 9.0,\n",
       " 10.0,\n",
       " 12.0,\n",
       " 10.0,\n",
       " 8.0,\n",
       " 11.0,\n",
       " 12.0,\n",
       " 13.0,\n",
       " 10.0,\n",
       " 14.0,\n",
       " 11.0,\n",
       " 10.0,\n",
       " 13.0,\n",
       " 10.0,\n",
       " 10.0,\n",
       " 10.0,\n",
       " 16.0,\n",
       " 9.0,\n",
       " 13.0,\n",
       " 12.0,\n",
       " 12.0,\n",
       " 11.0,\n",
       " 15.0,\n",
       " 9.0,\n",
       " 12.0,\n",
       " 8.0,\n",
       " 9.0,\n",
       " 11.0,\n",
       " 9.0,\n",
       " 11.0,\n",
       " 11.0,\n",
       " 15.0,\n",
       " 10.0,\n",
       " 10.0,\n",
       " 10.0,\n",
       " 16.0,\n",
       " 10.0,\n",
       " 11.0,\n",
       " 10.0,\n",
       " 13.0,\n",
       " 9.0,\n",
       " 10.0,\n",
       " 9.0,\n",
       " 17.0,\n",
       " 11.0,\n",
       " 10.0,\n",
       " 16.0,\n",
       " 8.0,\n",
       " 10.0,\n",
       " 9.0,\n",
       " 10.0,\n",
       " 8.0,\n",
       " 9.0,\n",
       " 10.0,\n",
       " 15.0,\n",
       " 11.0,\n",
       " 10.0,\n",
       " 12.0,\n",
       " 9.0,\n",
       " 16.0,\n",
       " 9.0,\n",
       " 10.0,\n",
       " 12.0,\n",
       " 13.0,\n",
       " 9.0,\n",
       " 11.0,\n",
       " 10.0,\n",
       " 9.0,\n",
       " 13.0,\n",
       " 9.0,\n",
       " 17.0,\n",
       " 14.0,\n",
       " 14.0,\n",
       " 12.0,\n",
       " 11.0,\n",
       " 8.0,\n",
       " 11.0,\n",
       " 9.0,\n",
       " 9.0,\n",
       " 11.0,\n",
       " 13.0,\n",
       " 12.0,\n",
       " 11.0,\n",
       " 10.0,\n",
       " 16.0,\n",
       " 11.0,\n",
       " 9.0,\n",
       " 9.0,\n",
       " 12.0,\n",
       " 10.0,\n",
       " 11.0,\n",
       " 9.0,\n",
       " 10.0,\n",
       " 10.0,\n",
       " 12.0,\n",
       " 11.0,\n",
       " 9.0,\n",
       " 9.0,\n",
       " 10.0,\n",
       " 9.0,\n",
       " 9.0,\n",
       " 14.0,\n",
       " 10.0,\n",
       " 11.0,\n",
       " 15.0,\n",
       " 12.0,\n",
       " 11.0,\n",
       " 13.0,\n",
       " 12.0,\n",
       " 12.0,\n",
       " 11.0,\n",
       " 10.0,\n",
       " 11.0,\n",
       " 13.0,\n",
       " 10.0,\n",
       " 9.0,\n",
       " 15.0,\n",
       " 9.0,\n",
       " 9.0,\n",
       " 11.0,\n",
       " 10.0,\n",
       " 8.0,\n",
       " 11.0,\n",
       " 13.0,\n",
       " 10.0,\n",
       " 13.0,\n",
       " 9.0,\n",
       " 9.0,\n",
       " 10.0,\n",
       " 8.0,\n",
       " 11.0,\n",
       " 10.0,\n",
       " 13.0,\n",
       " 10.0,\n",
       " 12.0,\n",
       " 8.0,\n",
       " 11.0,\n",
       " 11.0,\n",
       " 11.0,\n",
       " 13.0,\n",
       " 12.0,\n",
       " 13.0,\n",
       " 17.0,\n",
       " 9.0,\n",
       " 10.0,\n",
       " 14.0,\n",
       " 11.0,\n",
       " 9.0,\n",
       " 9.0,\n",
       " 10.0,\n",
       " 10.0,\n",
       " 15.0,\n",
       " 14.0,\n",
       " 13.0,\n",
       " 14.0,\n",
       " 15.0,\n",
       " 15.0,\n",
       " 11.0,\n",
       " 9.0,\n",
       " 10.0,\n",
       " 12.0,\n",
       " 18.0,\n",
       " 10.0,\n",
       " 11.0,\n",
       " 14.0,\n",
       " 11.0,\n",
       " 11.0,\n",
       " 11.0,\n",
       " 9.0,\n",
       " 8.0,\n",
       " 8.0,\n",
       " 10.0,\n",
       " 13.0,\n",
       " 14.0,\n",
       " 11.0,\n",
       " 13.0,\n",
       " 12.0,\n",
       " 11.0,\n",
       " 12.0,\n",
       " 11.0,\n",
       " 16.0,\n",
       " 13.0,\n",
       " 9.0,\n",
       " 10.0,\n",
       " 12.0,\n",
       " 13.0,\n",
       " 17.0,\n",
       " 15.0,\n",
       " 10.0,\n",
       " 10.0,\n",
       " 12.0,\n",
       " 8.0,\n",
       " 16.0,\n",
       " 12.0,\n",
       " 18.0,\n",
       " 10.0,\n",
       " 15.0,\n",
       " 9.0,\n",
       " 12.0,\n",
       " 9.0,\n",
       " 12.0,\n",
       " 11.0,\n",
       " 13.0,\n",
       " 8.0,\n",
       " 15.0,\n",
       " 13.0,\n",
       " 8.0,\n",
       " 9.0,\n",
       " 11.0,\n",
       " 12.0,\n",
       " 11.0,\n",
       " 20.0,\n",
       " 10.0,\n",
       " 10.0,\n",
       " 8.0,\n",
       " 8.0,\n",
       " 12.0,\n",
       " 9.0,\n",
       " 10.0,\n",
       " 10.0,\n",
       " 10.0,\n",
       " 11.0,\n",
       " 12.0,\n",
       " 10.0,\n",
       " 19.0,\n",
       " 10.0,\n",
       " 10.0,\n",
       " 9.0,\n",
       " 11.0,\n",
       " 9.0,\n",
       " 18.0,\n",
       " 15.0,\n",
       " 8.0,\n",
       " 12.0,\n",
       " 12.0,\n",
       " 18.0,\n",
       " 12.0,\n",
       " 9.0,\n",
       " 8.0,\n",
       " 10.0,\n",
       " 13.0,\n",
       " 11.0,\n",
       " 9.0,\n",
       " 18.0,\n",
       " 11.0,\n",
       " 18.0,\n",
       " 11.0,\n",
       " 12.0,\n",
       " 13.0,\n",
       " 13.0,\n",
       " 12.0,\n",
       " 18.0,\n",
       " 12.0,\n",
       " 10.0,\n",
       " 15.0,\n",
       " 10.0,\n",
       " 10.0,\n",
       " 9.0,\n",
       " 10.0,\n",
       " 10.0,\n",
       " 10.0,\n",
       " 12.0,\n",
       " 10.0,\n",
       " 17.0,\n",
       " 11.0,\n",
       " 10.0,\n",
       " 9.0,\n",
       " 10.0,\n",
       " 12.0,\n",
       " 10.0,\n",
       " 10.0,\n",
       " 9.0,\n",
       " 10.0,\n",
       " 11.0,\n",
       " 15.0,\n",
       " 11.0,\n",
       " 11.0,\n",
       " 9.0,\n",
       " 11.0,\n",
       " 10.0,\n",
       " 8.0,\n",
       " 10.0,\n",
       " 10.0,\n",
       " 8.0,\n",
       " 15.0,\n",
       " 11.0,\n",
       " 10.0,\n",
       " 15.0,\n",
       " 8.0,\n",
       " 13.0,\n",
       " 10.0,\n",
       " 12.0,\n",
       " 15.0,\n",
       " 9.0,\n",
       " 10.0,\n",
       " 11.0,\n",
       " 10.0,\n",
       " 9.0,\n",
       " 8.0,\n",
       " 14.0,\n",
       " 13.0,\n",
       " 9.0,\n",
       " 10.0,\n",
       " 10.0,\n",
       " 8.0,\n",
       " 9.0,\n",
       " 10.0,\n",
       " 11.0,\n",
       " 11.0,\n",
       " 12.0,\n",
       " 11.0,\n",
       " 9.0,\n",
       " 14.0,\n",
       " 10.0,\n",
       " 18.0,\n",
       " 11.0,\n",
       " 9.0,\n",
       " 11.0,\n",
       " 12.0,\n",
       " 13.0,\n",
       " 11.0,\n",
       " 11.0,\n",
       " 9.0,\n",
       " 9.0,\n",
       " 16.0,\n",
       " 11.0,\n",
       " 10.0,\n",
       " 15.0,\n",
       " 11.0,\n",
       " 11.0,\n",
       " 13.0,\n",
       " 8.0,\n",
       " 9.0,\n",
       " 9.0,\n",
       " 9.0,\n",
       " 10.0,\n",
       " 11.0,\n",
       " 14.0,\n",
       " 14.0,\n",
       " 12.0,\n",
       " 10.0,\n",
       " 10.0,\n",
       " 10.0,\n",
       " 11.0,\n",
       " 10.0,\n",
       " 15.0,\n",
       " 14.0,\n",
       " 10.0,\n",
       " 12.0,\n",
       " 12.0,\n",
       " 13.0,\n",
       " 10.0,\n",
       " 12.0,\n",
       " 10.0,\n",
       " 11.0,\n",
       " 10.0,\n",
       " 9.0,\n",
       " 11.0,\n",
       " 8.0,\n",
       " 12.0,\n",
       " 9.0,\n",
       " 10.0,\n",
       " 11.0,\n",
       " 10.0,\n",
       " 11.0,\n",
       " 8.0,\n",
       " 10.0,\n",
       " 8.0,\n",
       " 15.0,\n",
       " 11.0,\n",
       " 12.0,\n",
       " 12.0,\n",
       " 10.0,\n",
       " 13.0,\n",
       " 21.0,\n",
       " 14.0,\n",
       " 11.0,\n",
       " 12.0,\n",
       " 12.0,\n",
       " 12.0,\n",
       " 12.0,\n",
       " 9.0,\n",
       " 12.0,\n",
       " 14.0,\n",
       " 17.0,\n",
       " 11.0,\n",
       " 19.0,\n",
       " 18.0,\n",
       " 15.0,\n",
       " 9.0,\n",
       " 9.0,\n",
       " 13.0,\n",
       " 10.0,\n",
       " 15.0,\n",
       " 14.0,\n",
       " 9.0,\n",
       " 10.0,\n",
       " 10.0,\n",
       " 9.0,\n",
       " 10.0,\n",
       " 9.0,\n",
       " 9.0,\n",
       " 11.0,\n",
       " 11.0,\n",
       " 9.0,\n",
       " 12.0,\n",
       " 9.0,\n",
       " 13.0,\n",
       " 9.0,\n",
       " 13.0,\n",
       " 11.0,\n",
       " 10.0,\n",
       " 10.0,\n",
       " 10.0,\n",
       " 11.0,\n",
       " 13.0,\n",
       " 10.0,\n",
       " 10.0,\n",
       " 10.0,\n",
       " 10.0,\n",
       " 9.0,\n",
       " 12.0,\n",
       " 15.0,\n",
       " 11.0,\n",
       " 11.0,\n",
       " 12.0,\n",
       " 10.0,\n",
       " 11.0,\n",
       " 9.0,\n",
       " 10.0,\n",
       " 10.0,\n",
       " 11.0,\n",
       " 9.0,\n",
       " 8.0,\n",
       " 9.0]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dqnModel.rewardsStore"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
