{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from time import sleep\n",
    "from IPython.display import clear_output as clear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 超参数建立\n",
    "\n",
    "# 0: Normal, 1: Treasure, 0: End\n",
    "envMap = [0, 0, 0, 0, 0, 1, 0, 1, 2]\n",
    "visMap = ['.', '*', '#']\n",
    "ACTION = ['left', 'right']\n",
    "EPSILOM = 0.5                                               # 贪心系数\n",
    "GAMMA = 0.9\n",
    "EPOCHS = 10                                                 # 训练次数\n",
    "LEARNING_RATE = 0.2\n",
    "qTable = np.zeros( ( len(envMap), len(ACTION) ) )               # 初始化 Q Table Q[s, a] -> 在 s 状态下 采取 a 行为的 Q 值\n",
    "STEPCOUNT = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment 建立 -> 一维迷宫问题\n",
    "\n",
    "def VisualMap():\n",
    "    '''\n",
    "    return: string of the map\n",
    "    '''\n",
    "    return [visMap[point] for point in envMap]\n",
    "\n",
    "def GetReward(state, action):\n",
    "    '''\n",
    "    param:\n",
    "        state: the coordinate of the agent\n",
    "        action: the action agent will take\n",
    "    return:\n",
    "        nextState: as the agent take the action, the nextState it will be\n",
    "        reward: this action's reward\n",
    "    '''\n",
    "    action = ACTION[action]\n",
    "    if action == 'left':\n",
    "        # 走不了\n",
    "        if state == 0:\n",
    "            nextState = state\n",
    "            reward = -1\n",
    "        # 正常走\n",
    "        elif envMap[state - 1] == 0:\n",
    "            nextState = state - 1\n",
    "            reward = 0\n",
    "        elif envMap[state - 1] == 1:\n",
    "            nextState = state - 1\n",
    "            reward = 1\n",
    "        else:\n",
    "            print(\"Error\")\n",
    "    elif action == 'right':\n",
    "        # 走到终点\n",
    "        if envMap[state + 1] == 2:\n",
    "            nextState = 'end'\n",
    "            reward = 3\n",
    "        # 正常走\n",
    "        elif envMap[state + 1] == 0:\n",
    "            nextState = state + 1\n",
    "            reward = 1\n",
    "        elif envMap[state + 1] == 1:\n",
    "            nextState = state + 1\n",
    "            reward = 2\n",
    "        else:\n",
    "            print(\"Error\")\n",
    "    else:\n",
    "        nextState = state\n",
    "        # 惩罚不走的\n",
    "        reward = -1\n",
    "    \n",
    "    return nextState, reward\n",
    "\n",
    "def UpdateEnv(state, action, stepCounter):\n",
    "    '''\n",
    "    function:\n",
    "        Visualize the gameplaying map\n",
    "    '''\n",
    "    originMap = VisualMap()\n",
    "    \n",
    "    # 标记 agent 所在位置\n",
    "\n",
    "    if state == 'end':\n",
    "        print(\"End!\")\n",
    "        print(\"Use %d steps\" % stepCounter)\n",
    "        return \n",
    "\n",
    "    originMap[state] = '+'\n",
    "    sleep(1)\n",
    "    clear()\n",
    "\n",
    "    print(originMap)\n",
    "    print(\"action:%s\" % ACTION[action])\n",
    "    print(\"Use %d steps\" % stepCounter)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q-Learning 模型建立\n",
    "\n",
    "def ChooseAction(table, state):\n",
    "    '''\n",
    "    param:\n",
    "        table: the q table\n",
    "        state: the state agent is in now\n",
    "    return:\n",
    "        action: the action based on the q table\n",
    "    '''\n",
    "\n",
    "    statesQ = table[state]\n",
    "\n",
    "    # 搜索\n",
    "    if np.random.uniform() > EPSILOM: \n",
    "        return np.random.randint(len(ACTION))\n",
    "    # 贪心\n",
    "    else:\n",
    "        return np.argmax(statesQ)\n",
    "\n",
    "def Train():\n",
    "    for _ in range(EPOCHS):\n",
    "        stepCounter = 0\n",
    "        endLoop = False\n",
    "        state = 0                               # search from the place 0\n",
    "        while not endLoop:\n",
    "            action = ChooseAction(qTable, state)\n",
    "            nextState, reward = GetReward(state, action)\n",
    "            # print(nextState, reward)\n",
    "            # print(qTable)\n",
    "            predictQ = qTable[state, action]    # 采取行动后需要迭代更新 Q 值\n",
    "\n",
    "            # 结束\n",
    "            if nextState == 'end':\n",
    "                endLoop = True\n",
    "                realQ = reward\n",
    "            else:\n",
    "                realQ = reward + GAMMA * np.max(qTable[state, :])\n",
    "\n",
    "            qTable[state, action] = LEARNING_RATE * (realQ - predictQ)\n",
    "            state = nextState\n",
    "            stepCounter += 1\n",
    "            # UpdateEnv(state, action, stepCounter)\n",
    "        STEPCOUNT.append(stepCounter)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.13707294  0.19607843]\n",
      " [ 0.02936132  0.19607843]\n",
      " [ 0.02936471  0.19607843]\n",
      " [ 0.02942231  0.19607843]\n",
      " [ 0.05882364  0.39215686]\n",
      " [ 0.02941178  0.19607843]\n",
      " [ 0.22571992  0.39215686]\n",
      " [ 0.07344     0.49999995]\n",
      " [ 0.          0.        ]]\n",
      "[48, 14, 24, 15, 14, 10, 16, 8, 18, 8]\n"
     ]
    }
   ],
   "source": [
    "Train()\n",
    "print(qTable)\n",
    "print(STEPCOUNT)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
